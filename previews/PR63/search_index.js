var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"M. Besançon, T. Papamarkou, D. Anthoff, A. Arslan, S. Byrne, D. Lin and J. Pearson. Distributions.jl: Definition and Modeling of Probability Distributions in the JuliaStats Ecosystem. Journal of Statistical Software 98, 1–30 (2021).\n\n\n\nM. Sklar. Fonctions de repartition a n dimensions et leurs marges. Publ. inst. statist. univ. Paris 8, 229–231 (1959).\n\n\n\nH. Joe. Dependence Modeling with Copulas (CRC Press, 2015).\n\n\n\nS.-K. Au and J. L. Beck. Estimation of Small Failure Probabilities in High Dimensions by Subset Simulation. Probabilistic Engineering Mechanics 16, 263–277 (2001).\n\n\n\nK. Zuev. Subset Simulation Method for Rare Event Estimation: An Introduction. In: Encyclopedia of Earthquake Engineering (Springer, Berlin, Heidelberg, 2013).\n\n\n\nS.-K. Au and E. Patelli. Rare Event Simulation in Finite-Infinite Dimensional Space. Reliability Engineering & System Safety 148, 67–77 (2016).\n\n\n\nN. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller and E. Teller. Equation of State Calculations by Fast Computing Machines. The Journal of Chemical Physics 21, 1087–1092 (1953).\n\n\n\nW. K. Hastings. Monte Carlo Sampling Methods Using Markov Chains and Their Applications. Biometrika 57, 97–109 (1970).\n\n\n\nE. Patelli and S. K. Au. Efficient Monte Carlo Algorithm for Rare Failure Event Simulation. In: 12th International Conference on Applications of Statistics and Probability in Civil Engineering, ICASP 2012 (University of British Columbia, Jul 2015).\n\n\n\n","category":"page"},{"location":"examples/hpc/#High-Performance-Computing","page":"High Performance Computing","title":"High Performance Computing","text":"","category":"section"},{"location":"examples/hpc/#OpenMC-TBR-uncertainty","page":"High Performance Computing","title":"OpenMC TBR uncertainty","text":"","category":"section"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"In this example, we will run OpenMC, to compute the tritium breeding ratio (TBR) uncertainty, by varying material and geometric properties. This example was taken from the Fusion Neutronics Workshop.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"At first we need to create an array of random variables, that will be used when evaluating the points that our design produces.It will also define the range of the function we want the design to fit. This is also a good time to declare the function that we are working with.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Here we will vary the model's Li6 enrichment, and the radius of the tungsten layer.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"using UncertaintyQuantification\n\nE = RandomVariable(Uniform(0.3, 0.7), :E)\nR1 = RandomVariable(Uniform(8, 14), :R1)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Source/Extra files are expected to be in this folder.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"sourcedir = joinpath(pwd(), \"demo/models\")","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"These files will be rendered through Mustache.jl and have values injected.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"sourcefile = \"openmc_TBR.py\"","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Dictionary to map format Strings (Formatting.jl) to variables.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"numberformats = Dict(:E => \".8e\")","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"UQ will create subfolders in here to run the solver and store the results.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"workdir = joinpath(pwd(), \"openmc_TBR\")","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"note: Note\nIf Slurm is to run the jobs on multiple nodes, all the above folders must be shared by the computing nodes.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Read output file and compute the Tritium breeding ratio. An extractor is based the working directory for the current sample.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"TBR = Extractor(base -> begin\n    file = joinpath(base, \"openmc.out\")\n    line = readline(file)\n    tbr = parse(Float64, split(line, \" \")[1])\n\n    return tbr\nend, :TBR)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"In this example, an OpenMC model is built and run using the Python API. We therefore specify the python3` command, and the python file to run.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"openmc = Solver(\n    \"python3\", # path to python3 binary\n    \"openmc_TBR.py\";\n    args=\"\", # (optional) extra arguments passed to the solver\n)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Slurm interface, passing required machine information. Note in extras, we specify the commands we require to run the model (for example, loading modulefiles or data).","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"slurm = SlurmInterface(;\n    jobname=\"UQ_slurm\",\n    account=\"EXAMPLE-0001-CPU\",\n    partition=\"cpu_partition\",\n    nodes=1,\n    ntasks=1,\n    throttle=50,\n    time=\"00:05:00\", # Maximum time per simulation\n    extras=[\"module load openmc\", \"source ~/.virtualenvs/openmc/bin/activate\"],\n)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"With the SlurmInterface defined we can assemble the ExternalModel.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"ext = ExternalModel(\n    sourcedir,\n    sourcefile,\n    TBR,\n    openmc;\n    workdir=workdir,\n    formats=numberformats,\n    scheduler=slurm,\n)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Specify a limitstate function, negative value consititutes failure. Here we are interested in P(TBR <= 1).","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"function limitstate(df)\n    return reduce(vcat, df.TBR) .- 1\nend","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Finally, we can run a Monte Carlo simulation to obtain the probability of failure.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"@time pf, σ, samples = probability_of_failure(ext, limitstate, [E, R1], MonteCarlo(5000))","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"The simulation results in pf = 0.0064 and σ = 0.001127 for 5000 samples. Also TBR mean = 1.2404114576444423, TBR std = 0.10000460056126671, and TBR 95%: [1.0379178446904211, 1.4130216792418262].","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"We can also obtain the probability of failure with Subset simulation.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"subset = SubSetInfinity(800, 0.1, 10, 0.5)\n\n@time pf, std, samples = probability_of_failure(ext, limitstate, [E, R1], subset)\nprintln(\"Probability of failure: $pf\")","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"This results in pf = 0.0053 and σ = 0.001133 for 2400 samples.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"This page was generated using Literate.jl.","category":"page"},{"location":"manual/metamodels/#Metamodels","page":"Metamodelling","title":"Metamodels","text":"","category":"section"},{"location":"manual/metamodels/#Design-Of-Experiments","page":"Metamodelling","title":"Design Of Experiments","text":"","category":"section"},{"location":"manual/metamodels/","page":"Metamodelling","title":"Metamodelling","text":"Design Of Experiments (DOE) offers various designs that can be used for creating a model of a given system. The core idea is to evaluate significant points of the system in order to obtain a sufficient model while keeping the effort to achieve this relatively low. Depending on the parameters, their individual importance and interconnections, different designs may be adequate.","category":"page"},{"location":"manual/metamodels/","page":"Metamodelling","title":"Metamodelling","text":"The ones implemented here are TwoLevelFactorial, FullFactorial, FractionalFactorial, CentralComposite, BoxBehnken and PlackettBurman.","category":"page"},{"location":"manual/metamodels/#Response-Surface","page":"Metamodelling","title":"Response Surface","text":"","category":"section"},{"location":"manual/metamodels/","page":"Metamodelling","title":"Metamodelling","text":"A Response Surface is a simple polynomial surrogate model. It can be trained by providing it with evaluated points of a function or any of the aforementioned experimental designs.","category":"page"},{"location":"manual/reliability/#Reliability-Analysis","page":"Reliability Analysis","title":"Reliability Analysis","text":"","category":"section"},{"location":"manual/reliability/#Subset-Simulation","page":"Reliability Analysis","title":"Subset Simulation","text":"","category":"section"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Subset simulation [4] is an advanced simulation technique for the estimation of small failure probabilities. Here we solve a simple problem where the response y depends on two independent random variables x_1 and x_2 following a standard normal distribution. The simple linear model is defined by","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"y(x_1x_2) = x_1 + x_2","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"with the failure domain","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"F = (x_1 x_2)  x_1 + x_2  9","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"The analytical probability of failure can be calculated as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"pf = 1 - Phi(frac9sqrt(2)) approx 1 times 10^-10","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"This example is taken from [5].","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"In order to solve this, we start by creating the two random variables and group them in a vector inputs.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"using UncertaintyQuantification, DataFrames # hide\nusing Random; Random.seed!(8128) # hide\nx1 = RandomVariable(Normal(), :x1)\nx2 = RandomVariable(Normal(), :x2)\ninputs = [x1, x2]","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Next we define the model as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"y = Model(df -> df.x1 + df.x2, :y)\nnothing # hide","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"where the first input is our function (which must accept a DataFrame) and the second the Symbol for the output variable.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"To estimate a failure probability we need a performance which is negative if a failure occurs.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"function g(df::DataFrame)\n    return 9 .- df.y\nend\nnothing # hide","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Finally, we create the SubSetSimulation object and compute the probability of failure using a standard Gaussian proposal PDF. The value for the target probability of failure at each intermediate level is set to 01 which is generally accepted as the optimal value.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"subset = SubSetSimulation(1000, 0.1, 10, Normal())\npf, std, samples = probability_of_failure(y, g, inputs, subset)\n\nprintln(\"Probability of failure: $pf\")","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Alternatively, instead of using the standard Subset simulation algorithm (which internally uses Markov Chain Monte Carlo), we can use SubSetInfinity to compute the probability of failure, see [6]. Here we use a standard deviation of 05 to create the proposal samples for the next level.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"subset = SubSetInfinity(1000, 0.1, 10, 0.5)\npf, std, samples = probability_of_failure(y, g, inputs, subset)\n\nprintln(\"Probability of failure: $pf\")","category":"page"},{"location":"api/randomvariable/#RandomVariable","page":"RandomVariable","title":"RandomVariable","text":"","category":"section"},{"location":"api/randomvariable/#Index","page":"RandomVariable","title":"Index","text":"","category":"section"},{"location":"api/randomvariable/","page":"RandomVariable","title":"RandomVariable","text":"Pages = [\"randomvariable.md\"]\nModule = [\"UncertaintyQuantification\"]","category":"page"},{"location":"api/randomvariable/#Type","page":"RandomVariable","title":"Type","text":"","category":"section"},{"location":"api/randomvariable/","page":"RandomVariable","title":"RandomVariable","text":"RandomVariable","category":"page"},{"location":"api/randomvariable/#UncertaintyQuantification.RandomVariable","page":"RandomVariable","title":"UncertaintyQuantification.RandomVariable","text":"RandomVariable(dist::UnivariateDistribution, name::Symbol)\n\nDefines a random variable, with a univariate distribution from Distributions.jl and a name.\n\nExamples\n\njulia> RandomVariable(Normal(), :x)\nRandomVariable(Normal{Float64}(μ=0.0, σ=1.0), :x)\n\njulia> RandomVariable(Exponential(1), :x)\nRandomVariable(Exponential{Float64}(θ=1.0), :x)\n\n\n\n\n\n","category":"type"},{"location":"api/randomvariable/#Functions","page":"RandomVariable","title":"Functions","text":"","category":"section"},{"location":"api/randomvariable/","page":"RandomVariable","title":"RandomVariable","text":"sample(rv::RandomVariable, n::Integer)","category":"page"},{"location":"api/randomvariable/#UncertaintyQuantification.sample-Tuple{RandomVariable, Integer}","page":"RandomVariable","title":"UncertaintyQuantification.sample","text":"sample(rv::RandomVariable, n::Integer)\n\nGenerates n samples from a random variable. Returns a DataFrame.\n\nExamples\n\nSee also: RandomVariable\n\n\n\n\n\n","category":"method"},{"location":"examples/metamodels/#Metamodels","page":"Metamodels","title":"Metamodels","text":"","category":"section"},{"location":"examples/metamodels/#Design-Of-Experiments","page":"Metamodels","title":"Design Of Experiments","text":"","category":"section"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"Design Of Experiments (DOE) offers various designs that can be used for creating a model of a given system. The core idea is to evaluate significant points of the system in order to obtain a sufficient model while keeping the effort to achieve this relatively low. Depending on the parameters, their individual importance and interconnections, different designs may be adequate.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"The ones implemented here are TwoLevelFactorial, FullFactorial, FractionalFactorial, CentralComposite and BoxBehnken.","category":"page"},{"location":"examples/metamodels/#Response-Surface","page":"Metamodels","title":"Response Surface","text":"","category":"section"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"A Response Surface is a structure used for modeling.     It can be trained by providing it with evaluated points of a function.     It will then, using polynomial regression, compute a model of that function.","category":"page"},{"location":"examples/metamodels/#Example","page":"Metamodels","title":"Example","text":"","category":"section"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"In this example, we will model the following test function (known as Himmelblau's function) newlinein the range x1 x2  -5 5. It is defined as newline","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"f(x1 x2) = (x1^2 + x2 - 11)^2 + (x1 + x2^2 - 7)^2","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":".","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"using Plots #hide\na = range(-5, 5; length=1000)   #hide\nb = range(5, -5; length=1000)   #hide\nhimmelblau_f(x1, x2) = (x1^2 + x2 - 11)^2 + (x1 + x2^2 - 7)^2 #hide\ns1 = surface(a, b, himmelblau_f; plot_title=\"Himmelblau's function\")   #hide","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"At first we need to create an array of random variables, that will be used when evaluating the points that our desgin produces. It will also define the range of the function we want the design to fit. This is also a good time to declare the function that we are working with.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"using UncertaintyQuantification\n\nx = RandomVariable.(Uniform(-5, 5), [:x1, :x2])\n\nhimmelblau = Model(\n    df -> (df.x1 .^ 2 .+ df.x2 .- 11) .^ 2 .+ (df.x1 .+ df.x2 .^ 2 .- 7) .^ 2, :y\n)","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"Our next step is to chose the design we want to use and if required, set the parameters to the values we need or want. In this example, we are using a FullFactorial design:","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"design = FullFactorial([5, 5])","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"After that, we call the sample function with our design. This produces a matrix containing the points of our design fitted to the range defined via the RandomVariables. Wer then evaluate the function we want to model in these points and use the resulting data to train a ResponseSurface. The ResponseSurface uses regression to fit a polynomial function to the given datapoints. That functions degree is set as an Integer in the constructor.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"note: Note\nThe choice of the degree and the design and its parameters may be crucial to obtaining a sufficient model.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"training_data = sample(x, design)\nevaluate!(himmelblau, training_data)\nrs = ResponseSurface(training_data, :y, 4)\n\ntest_data = sample(x, 1000)\nevaluate!(rs, test_data)","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"To evaluate the ResponseSurfaceuse evaluate!(rs::ResponseSurface, data::DataFrame) with the dataframe containing the points you want to evaluate.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"The model in this case has an mse of about 1e-26 and looks like this in comparison to the original:","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"f(x1, x2) = map(m -> m([x1, x2]), rs.monomials') * rs.β #hide\n\ns2 = surface(a, b, f; plot_title=\"Response Surface\", plot_titlefontsize=16) #hide\nsurface(s1, s2; layout=(1, 2), legend=false, size=(800, 400))  #hide","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/inputs/#Inputs","page":"Inputs","title":"Inputs","text":"","category":"section"},{"location":"api/inputs/","page":"Inputs","title":"Inputs","text":"General functions operating on a collection of inputs defined as subtypes of UQInput.","category":"page"},{"location":"api/inputs/#Index","page":"Inputs","title":"Index","text":"","category":"section"},{"location":"api/inputs/","page":"Inputs","title":"Inputs","text":"Pages = [\"inputs.md\"]\nModule = [\"UncertaintyQuantification\"]","category":"page"},{"location":"api/inputs/#Functions","page":"Inputs","title":"Functions","text":"","category":"section"},{"location":"api/inputs/","page":"Inputs","title":"Inputs","text":"sample(inputs::Array{<:UQInput}, n::Integer)","category":"page"},{"location":"manual/hpc/#High-performance-computing","page":"High Performance Computing","title":"High performance computing","text":"","category":"section"},{"location":"manual/hpc/#Slurm-job-arrays","page":"High Performance Computing","title":"Slurm job arrays","text":"","category":"section"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"When sampling large simulation models, or complicated workflows, Julia's inbuilt parallelism is sometimes insufficient. Job arrays are a useful feature of the slurm scheduler which allow you to run many similar jobs, which differ by an index (for example a sample number). This allows UncertaintyQuantification.jl to run heavier simulations (for example, simulations requiring multiple nodes), by offloading model sampling to an HPC machine using slurm. This way, UncertaintyQuantification.jl can be started on a single worker, and the HPC machine handles the rest.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"For more information on job arrays, see: job arrays.","category":"page"},{"location":"manual/hpc/#SlurmInterface","page":"High Performance Computing","title":"SlurmInterface","text":"","category":"section"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"When SlurmInterface is passed to an ExternalModel, a slurm job array script is automatically generated and executed. Julia waits for this job to finish before extracting results and proceeding.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"using UncertaintyQuantification\n\nslurm = SlurmInterface(;\n    account=\"HPC_account_1\",\n    partition=\"CPU_partition\",\n    jobname=\"UQ_array\",\n    nodes=1,\n    ntasks=32,\n    throttle=50,\n    extras=[\"load python3\"],\n    time=\"01:00:00\",\n)","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Here account is your account (provided by your HPC admin/PI), and partition specifies the queue that jobs will be submitted to (ask admin if unsure). nodes and ntasks are the number of nodes and CPUs that your individual simulations requires. Depending on your HPC machine, each node has a specific number of CPUs. If your application requires more CPUs than are available per node, you can use multiple nodes.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"The parameter time specifies the maximum time that each simulation will be run for, before being killed.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"warning: Individual model runs VS overall batch\nnodes, ntasks, and time are parameters required for each individual model evaluation, not the entire batch. For example, if you are running a large FEM simulation that requires 100 CPUs to evaluate one sample, and your HPC machine has 50 CPUs per node, you would specify nodes = 2 and ntasks = 100.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"note: Compiling with MPI\nIf your model requires multiple nodes, it may be best to compile your application with MPI, if your model allows for it. Please check your application's documentation for compiling with MPI.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Any commands in extras will be executed before you model is run, for example loading any module files or data your model requires. Multiple commands can be passed: extras = [\"load python\", \"python3 get_data.py\"].","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"note: Note\nIf your extras command requires \"\" or $ symbols, they must be properly escaped as \\\"\\\" and \\$.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"The job array task throttle, which is the number of samples that will be run concurrently at any given time, is specified by throttle. For example, when running a MonteCarlo simulation with 2000 samples, and throttle = 50, 2000 model evaluations will be run in total, but only 50 at the same time. If left empty, your scheduler's default throttle will be used.","category":"page"},{"location":"manual/hpc/#Usage","page":"High Performance Computing","title":"Usage","text":"","category":"section"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"See examples/HPC for a detailed example.","category":"page"},{"location":"manual/bayesianupdating/#Bayesian-Updating","page":"Bayesian Updating","title":"Bayesian Updating","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Bayesian updating is a method of statistical inference where Bayes' theorem is used to update the probability distributions of model parameters based on prior beliefs and available data.","category":"page"},{"location":"manual/bayesianupdating/#Bayes'-Theorem","page":"Bayesian Updating","title":"Bayes' Theorem","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Bayes' theorem is defined as","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"P(thetaY) = fracP(Ytheta)P(theta)P(Y)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"where P(theta) is the prior distribution, describing prior belief on theta. P(Ytheta) is the likelihood function evaluating how the data Y supports our belief. This is a function of theta not Y. P(thetaY) is called the posterior probability and expresses the probability distribution of the updated belief under data Y. The term P(Y), often called the marginal likelihood, is the probability of the data. It can be calculated as the integral of the likelihood multiplied by the prior distribution over the sample space of theta","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"P(Y) = intP(Ytheta)P(theta) dtheta","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"This term serves as a normalizing constant for the posterior probability. However, as it can be difficult or even impossible to calculate it is often disregarded. Instead, only the product of likelihood and prior is used, as it is proportional to the posterior probability","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"P(thetaY) propto P(Ytheta)P(theta)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Based on this relationship, the posterior probability can be approximated without calculation of P(Y) using a variety of sampling methods. Classic approaches such as rejection sampling can be inefficient, especially for multivariate cases, because of high rejection rates. Instead, Metropolis et al., proposed the use of Markov chains to increase efficiency [7].","category":"page"},{"location":"manual/bayesianupdating/#Markov-Chain-Monte-Carlo","page":"Bayesian Updating","title":"Markov Chain Monte Carlo","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Markov chains are sequences of variables, where each variable is dependent on the last. In a discrete space Omega the series of random variables X_1X_2ldotsX_t is called a Marko chain if","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"p(X_t=x_tX_t-1=x_t-1ldotsX_1=x_1) = p(X_t=x_tX_t-1=x_t-1)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"A Markov chain is called ergodic or irreducible, when it is possible to reach each state from every other state with a positive probability. Markov chains that are ergodic and time-homogeneous, i.e. the probability between states doesn't depend on time, have a unique stationary distribution such that","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"pi(y) = sum_xinOmegaP(yx)pi(x)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The goal of Markov chain Monte Carlo (MCMC) sampling methods is to construct a Markov chain, whose stationary distribution is equal to the posterior distribution of Bayes' theorem. This will result in samples generated from the Markov chain being equivalent to random samples of the desired distribution. The very first MCMC algorithm is the Metropolis-Hastings (MH) Algorithm.","category":"page"},{"location":"manual/bayesianupdating/#Metropolis-Hastings","page":"Bayesian Updating","title":"Metropolis Hastings","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The Metropolis-Hastings algorithm, in its generalized form, was published in 1970 by W. K. Hastings [8]. The MH algorithm is a random-walk algorithm that provides a selection criteria for choosing the next sample (\\theta{i+1}) in a Markov chain. This is done through a so-called proposal distribution q(\\theta{i+1}|\\thetai)$ which is well known and relatively easy to sample from. Usually, symmetric proposal distributions centred at \\thetai$ are used which makes the Normal and Uniform distributions ideal candidates. A candidate sample theta^* is sampled from the proposal distribution and accepted with probability alpha","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"alpha = minleft1fracP(theta^*Y)P(theta_iY)cdotfracq(theta_itheta^*)q(theta^*theta_i)right","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Substituting the posterior with Bayes' theorem yields","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"alpha = minleft1fracP(Ytheta^*)cdotP(theta^*)P(Y)P(Ytheta_i)cdotP(theta_i)P(Y)cdotfracq(theta_itheta^*)q(theta^*theta_i)right","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Note, how the normalization constant P(Y) cancels out. Because of the symmetry q(theta_itheta^*) = q(theta^*theta_i) the acceptance probability simplifies to","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"alpha = minleft1fracP(theta^*Y)P(theta_iY)right","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"In practice, a random number r sim U(01) is sampled, and the candidate sample is accepted if a leq r.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"As an example consider a data sequence Y as the outcome of 100 Bernoulli trials with unknown success probability p (here p=0.8).","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"using UncertaintyQuantification # hide\n n = 100\n Y = rand(n) .<= 0.8\n return nothing # hide","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The likelihood function which, similar to a Model must accept a DataFrame, follows a Binomial distribution. And the prior is chosen as a beta distribution with alpha=beta=1. It is often beneficial to use the log-likelihood and log-prior for numerical reasons.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"    function loglikelihood(df)\n            return [\n                sum(logpdf.(Binomial.(n, df_i.p), sum(Y))) for df_i in eachrow(df)\n            ]\n        end\n\nlogprior = df -> logpdf.(Beta(1,1), df.p)\nreturn nothing # hide","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"UncertaintyQuantification.jl implements a variant of the MH algorithm known as single-component Metropolis-Hastings, where the proposal and acceptance step is performed independently for each dimension. To run the algorithm, we must first define the SingleComponentMetropolisHastings object which requires the UnivariateDistribution proposal, a NamedTuple x0 which defines the starting point of the Markov chain, the number of samples and the number of burn-in samples. The burn-in samples are used to start the chain but later discarded.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"    proposal = Normal(0, 0.2)\n    x0 = (;p=0.5)\n    n_samples= 2000\n    burnin = 500\n\n    mh = SingleComponentMetropolisHastings(proposal, x0, n_samples, burnin)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The final optional argument islog=true can be omitted when passing the log-likelihood and log-prior. When set to false, the algorithm will automatically compute the log for both functions. Finally, the algorithm is executed using the bayesianupdating function. This function returns the samples and the average acceptance rate.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"\nmh_samples, α   = bayesianupdating(logprior, loglikelihood, mh)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"using Plots # hide\nhistogram(mh_samples.p, bin = 100, label = \"MH\"; xlabel=\"p\")","category":"page"},{"location":"manual/bayesianupdating/#Transitional-Markov-Chain-Monte-Carlo","page":"Bayesian Updating","title":"Transitional Markov Chain Monte Carlo","text":"","category":"section"},{"location":"api/simulations/#Simulations","page":"Simulations","title":"Simulations","text":"","category":"section"},{"location":"api/simulations/","page":"Simulations","title":"Simulations","text":"Various Monte Carlo based simulations for a wide range of applications.","category":"page"},{"location":"api/simulations/#Index","page":"Simulations","title":"Index","text":"","category":"section"},{"location":"api/simulations/","page":"Simulations","title":"Simulations","text":"Pages = [\"simulations.md\"]\nModule = [\"UncertaintyQuantification\"]","category":"page"},{"location":"api/simulations/#Types","page":"Simulations","title":"Types","text":"","category":"section"},{"location":"api/simulations/","page":"Simulations","title":"Simulations","text":"SubSetSimulation\nSubSetInfinity","category":"page"},{"location":"api/simulations/#UncertaintyQuantification.SubSetSimulation","page":"Simulations","title":"UncertaintyQuantification.SubSetSimulation","text":"SubSetSimulation(n::Integer, target::Float64, levels::Integer, proposal::UnivariateDistribution)\n\nDefines the properties of a Subset simulation where n is the number of initial samples, target is the target probability of failure at each level, levels is the maximum number of levels and proposal is the proposal distribution for the markov chain monte carlo.\n\nExamples\n\njulia> SubSetSimulation(100, 0.1, 10, Uniform(-0.2, 0.2))\nSubSetSimulation(100, 0.1, 10, Uniform{Float64}(a=-0.2, b=0.2))\n\nReferences\n\n[4]\n\n\n\n\n\n","category":"type"},{"location":"api/simulations/#UncertaintyQuantification.SubSetInfinity","page":"Simulations","title":"UncertaintyQuantification.SubSetInfinity","text":"SubSetInfinity(n::Integer, target::Float64, levels::Integer, s::Real)\n\nDefines the properties of a Subset-∞ simulation where n is the number of initial samples, target is the target probability of failure at each level, levels is the maximum number of levels and s is the standard deviation for the proposal samples.\n\nExamples\n\njulia> SubSetInfinity(100, 0.1, 10, 0.5)\nSubSetInfinity(100, 0.1, 10, 0.5)\n\nReferences\n\n[6]\n\n[9]\n\n\n\n\n\n","category":"type"},{"location":"api/parameter/#Parameter","page":"Parameter","title":"Parameter","text":"","category":"section"},{"location":"api/parameter/#Index","page":"Parameter","title":"Index","text":"","category":"section"},{"location":"api/parameter/","page":"Parameter","title":"Parameter","text":"Pages = [\"parameter.md\"]\nModule = [\"UncertaintyQuantification\"]","category":"page"},{"location":"api/parameter/#Type","page":"Parameter","title":"Type","text":"","category":"section"},{"location":"api/parameter/","page":"Parameter","title":"Parameter","text":"Parameter","category":"page"},{"location":"api/parameter/#UncertaintyQuantification.Parameter","page":"Parameter","title":"UncertaintyQuantification.Parameter","text":"Parameter(value::Real, name::Symbol)\n\nDefines a parameter value (scalar), with an input value and a name.\n\nExamples\n\njulia> Parameter(3.14, :π)\nParameter(3.14, :π)\n\n\n\n\n\n","category":"type"},{"location":"api/polyharmonicspline/#PolyharmonicSpline","page":"PolyharmonicSpline","title":"PolyharmonicSpline","text":"","category":"section"},{"location":"api/polyharmonicspline/#Index","page":"PolyharmonicSpline","title":"Index","text":"","category":"section"},{"location":"api/polyharmonicspline/","page":"PolyharmonicSpline","title":"PolyharmonicSpline","text":"    Pages = [\"polyharmonicspline.md\"]\n    Module = [\"UncertaintyQuantification\"]","category":"page"},{"location":"api/polyharmonicspline/#Type","page":"PolyharmonicSpline","title":"Type","text":"","category":"section"},{"location":"api/polyharmonicspline/","page":"PolyharmonicSpline","title":"PolyharmonicSpline","text":"PolyharmonicSpline","category":"page"},{"location":"api/polyharmonicspline/#UncertaintyQuantification.PolyharmonicSpline","page":"PolyharmonicSpline","title":"UncertaintyQuantification.PolyharmonicSpline","text":"PolyharmonicSpline(data::DataFrame, k::Int64, output::Symbol)\n\nCreates a polyharmonic spline that is trained by given data.\n\n#Examples\n\njulia> data = DataFrame(x = 1:10, y = [1, -5, -10, -12, -8, -1, 5, 12, 23, 50]);\n\njulia> PolyharmonicSpline(data, 2, :y) |> DisplayAs.withcontext(:compact => true)\nPolyharmonicSpline([1.14733, -0.449609, 0.0140379, -1.02859, -0.219204, 0.900367, 0.00895592, 1.07145, -5.33101, 3.88628], [-112.005, 6.84443], [1.0; 2.0; … ; 9.0; 10.0;;], 2, [:x], :y)\n\n\n\n\n\n","category":"type"},{"location":"api/polyharmonicspline/#Functions","page":"PolyharmonicSpline","title":"Functions","text":"","category":"section"},{"location":"api/polyharmonicspline/","page":"PolyharmonicSpline","title":"PolyharmonicSpline","text":"evaluate!(ps::PolyharmonicSpline, df::DataFrame)","category":"page"},{"location":"api/polyharmonicspline/#UncertaintyQuantification.evaluate!-Tuple{PolyharmonicSpline, DataFrame}","page":"PolyharmonicSpline","title":"UncertaintyQuantification.evaluate!","text":"evaluate!(ps::PolyharmonicSpline, df::DataFrame)\n\nEvaluate given data using a previously contructed PolyharmonicSpline metamodel.\n\n#Examples\n\njulia> data = DataFrame(x = 1:10, y = [1, -5, -10, -12, -8, -1, 5, 12, 23, 50]);\n\njulia> ps = PolyharmonicSpline(data, 2, :y);\n\njulia> df = DataFrame( x = [2.5, 7.5, 12, 30]);\n\njulia> evaluate!(ps, df);\n\njulia> df.y |> DisplayAs.withcontext(:compact => true)\n4-element Vector{Float64}:\n  -7.75427\n   8.29083\n  84.4685\n 260.437\n\n\n\n\n\n","category":"method"},{"location":"api/slurm/#SlurmInterface","page":"SlurmInterface","title":"SlurmInterface","text":"","category":"section"},{"location":"api/slurm/#Index","page":"SlurmInterface","title":"Index","text":"","category":"section"},{"location":"api/slurm/","page":"SlurmInterface","title":"SlurmInterface","text":"Pages = [\"slurm.md\"]\nModule = [\"UncertaintyQuantification\"]","category":"page"},{"location":"api/slurm/#Type","page":"SlurmInterface","title":"Type","text":"","category":"section"},{"location":"api/slurm/","page":"SlurmInterface","title":"SlurmInterface","text":"SlurmInterface","category":"page"},{"location":"api/slurm/#UncertaintyQuantification.SlurmInterface","page":"SlurmInterface","title":"UncertaintyQuantification.SlurmInterface","text":"SlurmInterface(account::String, partition::String, nodes::Integer, ntasks::Integer, throttle::Integer, jobname::String, extras::Vector{String}, time::String)\n\nWhen SlurmInterface is passed to an ExternalModel, model evaluations are executed using slurm job arrays. This allows for heavier simulations or workflows to be sampled, without relying on Julia's native parallelism. SlurmInterface automatically generates a slurm job array script, and Julia waits for this job to finish before extracting results.\n\nWhen using SlurmInterface, you no longer need to load workers into Julia with addprocs(N), and the requested nodes / tasks those required by individual model evaluations. Use extras to specify anything that must be preloaded for your models to be executed (for example loading modules).\n\nThe throttle specifies the number of simulations in the job array which are run concurrently. I.e., if you perform MonteCarlo simulation with N=1000 samples, with throttle=200, it will run 1000 simulations in total, but only 200 at the same time. Your HPC scheduler (and admin) may be unhappy if you request too many concurrent jobs. If left empty, you scheduler's default throttle will be used.\n\nparameters\n\naccount   : the account to charge the computing time to\npartition : the partition to use\nnodes     : number of nodes per job\nntasks    : total number of cores\nthrottle  : the number of jobs to be run at the same time\njobname   : name of the job\nmempercpu : string, amount of RAM to give per cpu (default MB)\nextras    : instructions to be executed before the model is run, e.g. activating a python environment\ntime      : how long each job takes, e.g. 00:10:00 for 10 minutes per sample\n\nExamples\n\njulia> slurm = SlurmInterface(account = \"HPC_account_1\", partition = \"CPU_partition\", nodes = 1, ntasks = 32, throttle = 200, extras = [\"load python3\"], time = \"00:10:00\")\nSlurmInterface(\"HPC_account_1\", \"CPU_partition\", 1, 32, 200, \"UQ_array\", \"\", [\"load python3\"], \"00:10:00\")\n\n\n\n\n\n","category":"type"},{"location":"manual/gettingstarted/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Here we introduce the basic building blocks of UncertaintyQuantification. This includes the inputs such as Parameter or RandomVariable which will feed into any Model for a variety of different analyses. We will also present more advanced concepts including how to model dependencies between the inputs through copulas.","category":"page"},{"location":"manual/gettingstarted/#Inputs","page":"Getting Started","title":"Inputs","text":"","category":"section"},{"location":"manual/gettingstarted/#Parameters","page":"Getting Started","title":"Parameters","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A Parameter is defined as a constant scalar value. In addition to value the constructor also requires a Symbol by which it can later be identified in the Model. A Symbol is a Julia object which is often used as a name or label. Symbols are defined using the : prefix. Parameters represent constant deterministic values. As an example we define a Parameter representing the gravity of Earth.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"using UncertaintyQuantification # hide\ng = Parameter(9.81, :g)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Parameters are very handy when constants show up in the Model in multiple spaces. Instead of updating every instance in the Model, we can conveniently update the value by changing a single line.","category":"page"},{"location":"manual/gettingstarted/#Random-Variables","page":"Getting Started","title":"Random Variables","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A RandomVariable is essentially a wrapper around any UnivariateDistribution defined in the Distributions.jl package [1]. Similarly to the Parameter, the second argument to the constructor is a Symbol acting as a unique identifier. For example, a standard gaussian random variable is defined by passing Normal() and :x as arguments.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"x = RandomVariable(Normal(), :x)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A list of all possible distributions can be generated by executing subtypes(UnivariateDistribution) in the Julia REPL (read-eval-print loop). Note that, Distributions is re-exported from UncertaintyQuantification and no separate using statement is necessary. In addition, the most important methods of the UnivariateDistribution including pdf, cdf, and quantile, are also defined for the RandomVariable.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Random samples can be drawn from a RandomVariable by calling the sample method passing the random variable and the desired number of samples.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"samples = sample(x, 100) # sample(x, MonteCarlo(100))\nreturn nothing # hide","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"The sample method returns a DataFrame with the samples in a single column. When sampling from a Vector of random variables these invidivual columns are automatically merged into one unified DataFrame. By default, this will use stardard Monte Carlo simulation to obtain the samples. Alternatively, any of the quasi-Monte Carlo methods can be used instead.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"samples = sample(x, SobolSampling(100))\nsamples = sample(x, LatinHypercubeSampling(100))\nreturn nothing # hide","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Many of the advanced simulations, e.g. line sampling or subset simulation require mappings to (and from) the standard normal space, and these are exposed through the to_standard_normal_space! and to_physical_space! methods respectively. These operate on a DataFrame and as such can be applied to samples directly. The transformation is done in-place, i.e. no new DataFrame is returned. As such, in the following example, the samples end up exactly as they were in the beginning.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"to_standard_normal_space!(x, samples)\nto_physical_space!(x, samples)","category":"page"},{"location":"manual/gettingstarted/#Dependencies","page":"Getting Started","title":"Dependencies","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"UncertaintyQuantification supports modelling of dependencies through copulas. By using copulas, the modelling of the dependence structure is separated from the modelling of the univariate marginal distributions. The basis for copulas is given by Sklar's theorem [2]. It states that any multivariate distribution H in dimensions d geq 2 can be separated into its marginal distributions F_i and a copula function C $.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"H(x_1ldotsx_2) = C(F_1(x_1)ldotsF_d(x_d))","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"For a thorough discussion of copulas, see [3].","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"In line with Sklar's theorem we build the joint distribution of two dependent random variables by separately defining the marginal distributions.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"using UncertaintyQuantification # hide\nx = RandomVariable(Normal(), :x)\ny = RandomVariable(Uniform(), :y)\nmarginals = [x, y]\nreturn nothing # hide","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Next, we define the copula to model the dependence. UncertaintyQuantification supports Gaussian copulas for multivariate d geq 2 dependence. Here, we define a Gaussian copula by passing the correlation matrix and then build the JointDistribution from the copula and the marginals.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"cop = GaussianCopula([1 0.8; 0.8 1])\njoint = JointDistribution(marginals, cop)\nreturn nothing # hide","category":"page"},{"location":"manual/gettingstarted/#Models","page":"Getting Started","title":"Models","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"In this section we present the models included in UncertaintyQuantification. A model, in its most basic form, is a relationship between a set of input variables x in mathbbR^n_x and an output y in mathbbR. Currently, most models are assumed to return single-valued outputs. However, as seen later, the ExternalModel is capable of extracting an arbitrary number of outputs from a single run of an external solver.","category":"page"},{"location":"manual/gettingstarted/#Model","page":"Getting Started","title":"Model","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A Model is essentially a native Julia function operating on the previously defined inputs. Building a Model requires two things: a Function, which is internally passed a DataFrame containing the samples and must return a Vector containing the model response for each sample, and a Symbol which is the identifier used to add the model output into the DataFrame.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Suppose we wanted to define a Model which computes the distance from the origin of two variables x and y as z. We first define the function and then pass it to the Model.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"using UncertaintyQuantification, DataFrames # hide\nfunction z(df::DataFrame)\n  return @. sqrt(df.x^2 + df.y^2)\nend\nm = Model(z, :z)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"An alternative for a simple model such as this, is to directly pass an anonymous function to the Model.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"m = Model(df -> sqrt.(df.x.^2 .+ df.y.^2), :z)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"After defining it, a Model can be evaluated on a set of samples by calling the evaluate! method. This will add the model outcome to the DataFrame. Alternatively, the reponse can be obtained as a vector, by calling the Model as a function.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"samples = sample([x, y], MonteCarlo(1000))\nevaluate!(m, samples) # add to the DataFrame\noutput = m(samples) # return a Vector","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"However, most of the time manual evaluation of the Model will not be necessary as it is done internally by whichever analysis is performed.","category":"page"},{"location":"manual/gettingstarted/#ParallelModel","page":"Getting Started","title":"ParallelModel","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"With the basic Model it is up to the user to implement an efficient function which returns the model responses for all samples simultaneously. Commonly, this will involve vectorized operations as presented in the example. For more complex or longer running models, UncertaintyQuantification provides a simple ParallelModel. This model relies on the capabilites of the Distributed module, which is part of the standard library shipped with Julia. Without any present workers, the ParallelModel will evaluate its function in a loop for each sample. If one or more workers are present, it will automatically distribute the model evaluations. For this to work, UncertaintyQuantification must be loaded with the @everywhere macro in order to be loaded on all workers. In the following example, we first load Distributed and add four local workers. A simple model is then evaluated in parallel. Finally, the workers are removed.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"using Distributed\naddprocs(4) # add 4 local workers\n\n@everywhere using UncertaintyQuantification\n\nx = RandomVariable(Normal(), :x)\ny = RandomVariable(Normal(), :y)\n\nm = ParallelModel(df -> sqrt(df.x^2 .+ df.y^2), :z)\n\nsamples = sample([x, y], 1000)\nevaluate!(m, samples)\n\nrmprocs(workers()) # release the local workers","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"It is important to note, that the ParallelModel requires some overhead to distribute the function calls to the workers. Therefore it performs significantly slower than the standard Model with vectorized operations for a simple function as in this example.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"By using ClusterManagers.jl to add the workers, the ParallelModel can easily be run on an existing compute cluster such as Slurm.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"note: Note\nFor heavier external models or workflows in parallel on compute clusters, using SlurmInterface is recommended. See High Performance Computing.","category":"page"},{"location":"manual/gettingstarted/#ExternalModel","page":"Getting Started","title":"ExternalModel","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"The ExternalModel provides interaction with almost any third-party solver. The only requirement is, that the solver uses text-based input and output files in which the values sampled from the random variables can be injected for each individual run. The output quantities are then extracted from the files generated by the solver using one (or more) Extractor(s). This way, the simulation techniques included in this module, can be applied to advanced models in finite element software such as OpenSees or Abaqus.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"The first step in building the ExternalModel is to define the folder where the source files can be found as well as the working directory. Here, we assume that the source file for a simple supported beam model is located in a subdirectory of our current working directory. Similarly, the working directory for the solver is defined. In addition, we define the exact files where values need to be injected, and any extra files required. No values will be injected into the files specified as extra. In this example, no extra files are needed, so the variable is defined as an empty String vector.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"sourcedir = joinpath(pwd(), \"demo/models\")\nsourcefiles = [\"supported-beam.tcl\"]\nextrafiles = String[]\nworkdir = joinpath(pwd(), \"supported-beam\")","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Next, we must define where to inject values from the random variables and parameters into the input files. For this, we make use of the Mustache.jl and Formatting.jl modules. The values in the source file must be replaced by triple curly bracket expressions of the form {{{ :x }}},  where :x is the identifier of the RandomVariable or Parameter to be injected. For example, to inject the Young's modulus and density of an elastic isotropic material in OpenSees, one could write the following.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"nDMaterial ElasticIsotropic 1 {{{ :E }}} 0.25 {{{ :rho }}}","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"This identifies where to inject the values, but not in which format. For this reason, we define a Dict{Symbol, String} which maps the identifiers of the inputs to a Python-style format string. In order to inject our values in scientific notation with eight digits, we use the format string \".8e\". For any not explicitly defined Symbol we can include :* as a fallback.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"formats = Dict(:E => \".8e\",:rho => \".8e\", :* => \".12e\")","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"After formatting and injecting the values into the source file, it would look similar to this.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"nDMaterial ElasticIsotropic 1 9.99813819e+02 0.25 3.03176259e+00","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Now that the values are injected into the source files, the next step is to extract the desired output quantities. This is done using an Extractor. The Extractor is designed similarly to the Model in that it takes a Function and a Symbol as its parameters. However, where a DataFrame is passed to the Model, the working directoy for the currently evaluated sample is passed to the function of the Extractor. The user defined function must then extract the required values from the file and return them. Here, we make use of the DelimitedFiles module to extract the maximum absolute displacement from the output file that OpenSees generated.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"disp = Extractor(base -> begin\n  file = joinpath(base, \"displacement.out\")\n  data = readdlm(file, ' ')\n\n  return maximum(abs.(data[:, 2]))\nend, :disp)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"An arbitrary number of Extractor functions can be defined in order to extract multiple output values from the solver.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"The final step before building the model is to define the solver. The solver requires the path to the binary, and the input file. Optional command line arguments can be passed to the Solver through the args keyword. If the solver binary is not on the system path, the full path to the executable must be defined. Finally, the ExternalModel is assembled.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"opensees = Solver(\n  \"OpenSees\",\n  \"supported-beam.tcl\";\n  args = \"\"\n)\n\next = ExternalModel(\n  sourcedir, sourcefiles, disp, opensees; formats=numberformats, workdir=workdir, extras=extrafiles\n)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A full example of how to run a reliability analysis of a model defined in OpenSees can be found in the demo files.","category":"page"},{"location":"api/responsesurface/#ResponseSurface","page":"ResponseSurface","title":"ResponseSurface","text":"","category":"section"},{"location":"api/responsesurface/#Index","page":"ResponseSurface","title":"Index","text":"","category":"section"},{"location":"api/responsesurface/","page":"ResponseSurface","title":"ResponseSurface","text":"    Pages = [\"responcesurface.md\"]\n    Module = [\"UncertaintyQuantification\"]","category":"page"},{"location":"api/responsesurface/#Type","page":"ResponseSurface","title":"Type","text":"","category":"section"},{"location":"api/responsesurface/","page":"ResponseSurface","title":"ResponseSurface","text":"ResponseSurface","category":"page"},{"location":"api/responsesurface/#UncertaintyQuantification.ResponseSurface","page":"ResponseSurface","title":"UncertaintyQuantification.ResponseSurface","text":"ResponseSurface(data::DataFrame, dependendVarName::Symbol, deg::Int, dim::Int)\n\nCreates a response surface using polynomial least squares regression with given degree.\n\nExamples\n\njulia> data = DataFrame(x = 1:10, y = [1, 4, 10, 15, 24, 37, 50, 62, 80, 101]);\n\njulia> rs = ResponseSurface(data, :y, 2) |> DisplayAs.withcontext(:compact => true)\nResponseSurface([0.483333, -0.238636, 1.01894], :y, [:x], 2, Monomial{Commutative{CreationOrder}, Graded{LexOrder}}[1, x₁, x₁²])\n\n\n\n\n\n","category":"type"},{"location":"api/responsesurface/#Functions","page":"ResponseSurface","title":"Functions","text":"","category":"section"},{"location":"api/responsesurface/","page":"ResponseSurface","title":"ResponseSurface","text":"evaluate!(rs::ResponseSurface, data::DataFrame)","category":"page"},{"location":"api/responsesurface/#UncertaintyQuantification.evaluate!-Tuple{ResponseSurface, DataFrame}","page":"ResponseSurface","title":"UncertaintyQuantification.evaluate!","text":"evaluate!(rs::ResponseSurface, data::DataFrame)\n\nevaluating data by using a previously trained ResponseSurface.\n\nExamples\n\njulia> data = DataFrame(x = 1:10, y = [1, 4, 10, 15, 24, 37, 50, 62, 80, 101]);\n\njulia> rs = ResponseSurface(data, :y, 2);\n\njulia> df = DataFrame(x = [2.5, 11, 15]);\n\njulia> evaluate!(rs, df);\n\njulia> df.y |> DisplayAs.withcontext(:compact => true)\n3-element Vector{Float64}:\n   6.25511\n 121.15\n 226.165\n\n\n\n\n\n","category":"method"},{"location":"#UncertaintyQuantification.jl","page":"Home","title":"UncertaintyQuantification.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for uncertainty quantification. Current functionality includes:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Simulation-based reliability analysis\nMonte Carlo simulation\nQuasi Monte Carlo simulation (Sobol, Halton)\nLine Sampling\nSubset Simulation\nSensitivity analysis\nGradients\nSobol indices\nMetamodeling\nPolyharmonic splines\nResponse Surface\nThird-party solvers\nConnect to any solver by injecting random samples into source files\nHPC interfacing with slurm","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the latest release through the Julia package manager run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add UncertaintyQuantification\njulia> using UncertaintyQuantification","category":"page"},{"location":"","page":"Home","title":"Home","text":"or install the latest development version with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add UncertaintyQuantification#master\njulia> using UncertaintyQuantification","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Jasper Behrensdorf, Institute for Risk and Reliability, Leibniz University Hannover\nAnder Gray, Institute for Risk and Uncertainty, University of Liverpool","category":"page"},{"location":"#Related-packages","page":"Home","title":"Related packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"OpenCossan: Matlab-based toolbox for uncertainty quantification and management","category":"page"}]
}
