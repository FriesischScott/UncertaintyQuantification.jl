<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bayesian Updating · UncertaintyQuantification.jl</title><meta name="title" content="Bayesian Updating · UncertaintyQuantification.jl"/><meta property="og:title" content="Bayesian Updating · UncertaintyQuantification.jl"/><meta property="twitter:title" content="Bayesian Updating · UncertaintyQuantification.jl"/><meta name="description" content="Documentation for UncertaintyQuantification.jl."/><meta property="og:description" content="Documentation for UncertaintyQuantification.jl."/><meta property="twitter:description" content="Documentation for UncertaintyQuantification.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">UncertaintyQuantification.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../gettingstarted/">Getting Started</a></li><li><a class="tocitem" href="../reliability/">Reliability Analysis</a></li><li><a class="tocitem" href="../metamodels/">Metamodelling</a></li><li class="is-active"><a class="tocitem" href>Bayesian Updating</a><ul class="internal"><li><a class="tocitem" href="#Bayes&#39;-Theorem"><span>Bayes&#39; Theorem</span></a></li><li><a class="tocitem" href="#Markov-Chain-Monte-Carlo"><span>Markov Chain Monte Carlo</span></a></li></ul></li><li><a class="tocitem" href="../hpc/">High Performance Computing</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/metamodels/">Metamodels</a></li><li><a class="tocitem" href="../../examples/hpc/">High Performance Computing</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../api/inputs/">Inputs</a></li><li><a class="tocitem" href="../../api/parameter/">Parameter</a></li><li><a class="tocitem" href="../../api/randomvariable/">RandomVariable</a></li><li><a class="tocitem" href="../../api/responsesurface/">ResponseSurface</a></li><li><a class="tocitem" href="../../api/polyharmonicspline/">PolyharmonicSpline</a></li><li><a class="tocitem" href="../../api/simulations/">Simulations</a></li><li><a class="tocitem" href="../../api/slurm/">SlurmInterface</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Bayesian Updating</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bayesian Updating</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FriesischScott/UncertaintyQuantification.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FriesischScott/UncertaintyQuantification.jl/blob/master/docs/src/manual/bayesianupdating.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Bayesian-Updating"><a class="docs-heading-anchor" href="#Bayesian-Updating">Bayesian Updating</a><a id="Bayesian-Updating-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Updating" title="Permalink"></a></h1><p>Bayesian updating is a method of statistical inference where Bayes&#39; theorem is used to update the probability distributions of model parameters based on prior beliefs and available data.</p><h2 id="Bayes&#39;-Theorem"><a class="docs-heading-anchor" href="#Bayes&#39;-Theorem">Bayes&#39; Theorem</a><a id="Bayes&#39;-Theorem-1"></a><a class="docs-heading-anchor-permalink" href="#Bayes&#39;-Theorem" title="Permalink"></a></h2><p>Bayes&#39; theorem is defined as</p><p class="math-container">\[P(\theta|Y) = \frac{P(Y|\theta)P(\theta)}{P(Y)},\]</p><p>where <span>$P(\theta)$</span> is the prior distribution, describing prior belief on <span>$\theta$</span>. <span>$P(Y|\theta)$</span> is the likelihood function evaluating how the data Y supports our belief. This is a function of <span>$\theta$</span> not <span>$Y$</span>. <span>$P(\theta|Y)$</span> is called the posterior probability and expresses the probability distribution of the updated belief under data <span>$Y$</span>. The term <span>$P(Y)$</span>, often called the marginal likelihood, is the probability of the data. It can be calculated as the integral of the likelihood multiplied by the prior distribution over the sample space of <span>$\theta$</span></p><p class="math-container">\[P(Y) = \int{}P(Y|\theta)P(\theta), d\theta{}.\]</p><p>This term serves as a normalizing constant for the posterior probability. However, as it can be difficult or even impossible to calculate it is often disregarded. Instead, only the product of likelihood and prior is used, as it is proportional to the posterior probability</p><p class="math-container">\[P(\theta|Y) \propto P(Y|\theta)P(\theta).\]</p><p>Based on this relationship, the posterior probability can be approximated without calculation of <span>$P(Y)$</span> using a variety of sampling methods. Classic approaches such as rejection sampling can be inefficient, especially for multivariate cases, because of high rejection rates. Instead, Metropolis et al., proposed the use of Markov chains to increase efficiency [<a href="../../references/#metropolisEquationStateCalculations1953">7</a>].</p><h2 id="Markov-Chain-Monte-Carlo"><a class="docs-heading-anchor" href="#Markov-Chain-Monte-Carlo">Markov Chain Monte Carlo</a><a id="Markov-Chain-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Markov-Chain-Monte-Carlo" title="Permalink"></a></h2><p>Markov chains are sequences of variables, where each variable is dependent on the last. In a discrete space <span>$\Omega$</span> the series of random variables <span>$\{X_1,X_2,\ldots,X_t\}$</span> is called a Marko chain if</p><p class="math-container">\[p(X_t=x_t|X_{t-1}=x_{t-1},\ldots,X_1=x_1) = p(X_t=x_t|X_{t-1}=x_{t-1})\]</p><p>A Markov chain is called ergodic or irreducible, when it is possible to reach each state from every other state with a positive probability. Markov chains that are ergodic and time-homogeneous, i.e. the probability between states doesn&#39;t depend on time, have a unique stationary distribution such that</p><p class="math-container">\[\pi(y) = \sum_{x\in\Omega}P(y|x)\pi(x).\]</p><p>The goal of Markov chain Monte Carlo (MCMC) sampling methods is to construct a Markov chain, whose stationary distribution is equal to the posterior distribution of Bayes&#39; theorem. This will result in samples generated from the Markov chain being equivalent to random samples of the desired distribution. The very first MCMC algorithm is the Metropolis-Hastings (MH) Algorithm.</p><h3 id="Metropolis-Hastings"><a class="docs-heading-anchor" href="#Metropolis-Hastings">Metropolis Hastings</a><a id="Metropolis-Hastings-1"></a><a class="docs-heading-anchor-permalink" href="#Metropolis-Hastings" title="Permalink"></a></h3><p>The Metropolis-Hastings algorithm, in its generalized form, was published in 1970 by W. K. Hastings [<a href="../../references/#hastingsMonteCarloSampling1970">8</a>]. The MH algorithm is a random-walk algorithm that provides a selection criteria for choosing the next sample (\theta<em>{i+1}) in a Markov chain. This is done through a so-called proposal distribution q(\theta</em>{i+1}|\theta<em>i)$ which is well known and relatively easy to sample from. Usually, symmetric proposal distributions centred at \theta</em>i$ are used which makes the Normal and Uniform distributions ideal candidates. A candidate sample <span>$\theta^*$</span> is sampled from the proposal distribution and accepted with probability <span>$\alpha$</span></p><p class="math-container">\[\alpha = \min\left[1,\frac{P(\theta^*|Y)}{P(\theta_i|Y)}\cdot{}\frac{q(\theta_i|\theta^*)}{q(\theta^*|\theta_i)}\right].\]</p><p>Substituting the posterior with Bayes&#39; theorem yields</p><p class="math-container">\[\alpha = \min\left[1,\frac{P(Y|\theta^*)\cdot{}P(\theta^*)/P(Y)}{P(Y|\theta_i)\cdot{}P(\theta_i)/P(Y)}\cdot{}\frac{q(\theta_i|\theta^*)}{q(\theta^*|\theta_i)}\right].\]</p><p>Note, how the normalization constant <span>$P(Y)$</span> cancels out. Because of the symmetry <span>$q(\theta_i|\theta^*) = q(\theta^*|\theta_i)$</span> the acceptance probability simplifies to</p><p class="math-container">\[\alpha = \min\left[1,\frac{P(\theta^*|Y)}{P(\theta_i|Y)}\right].\]</p><p>In practice, a random number <span>$r \sim U(0,1)$</span> is sampled, and the candidate sample is accepted if <span>$a \leq r$</span>.</p><p>As an example consider a data sequence <code>Y</code> as the outcome of 100 Bernoulli trials with unknown success probability <code>p</code> (here p=0.8).</p><pre><code class="language-julia hljs"> n = 100
 Y = rand(n) .&lt;= 0.8</code></pre><p>The likelihood function which, similar to a <code>Model</code> must accept a <code>DataFrame</code>, follows a Binomial distribution. And the prior is chosen as a beta distribution with <span>$\alpha=\beta=1$</span>. It is often beneficial to use the log-likelihood and log-prior for numerical reasons.</p><pre><code class="language-julia hljs">    function loglikelihood(df)
            return [
                sum(logpdf.(Binomial.(n, df_i.p), sum(Y))) for df_i in eachrow(df)
            ]
        end

logprior = df -&gt; logpdf.(Beta(1,1), df.p)</code></pre><p><strong>UncertaintyQuantification.jl</strong> implements a variant of the MH algorithm known as single-component Metropolis-Hastings, where the proposal and acceptance step is performed independently for each dimension. To run the algorithm, we must first define the <code>SingleComponentMetropolisHastings</code> object which requires the <code>UnivariateDistribution</code> <code>proposal</code>, a <code>NamedTuple</code> <code>x0</code> which defines the starting point of the Markov chain, the number of samples and the number of burn-in samples. The burn-in samples are used to start the chain but later discarded.</p><pre><code class="language-julia hljs">    proposal = Normal(0, 0.2)
    x0 = (;p=0.5)
    n_samples= 2000
    burnin = 500

    mh = SingleComponentMetropolisHastings(proposal, x0, n_samples, burnin)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">SingleComponentMetropolisHastings(Normal{Float64}(μ=0.0, σ=0.2), (p = 0.5,), 2000, 500, true)</code></pre><p>The final optional argument <code>islog=true</code> can be omitted when passing the log-likelihood and log-prior. When set to <code>false</code>, the algorithm will automatically compute the <code>log</code> for both functions. Finally, the algorithm is executed using the <code>bayesianupdating</code> function. This function returns the samples and the average acceptance rate.</p><pre><code class="language-julia hljs">mh_samples, α   = bayesianupdating(logprior, loglikelihood, mh)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(<span class="sgr1">2000×1 DataFrame
  Row │ p
      │<span class="sgr90"> Float64
──────┼──────────
    1 │ 0.826745
    2 │ 0.829098
    3 │ 0.826567
    4 │ 0.826567
    5 │ 0.817685
    6 │ 0.817685
    7 │ 0.817685
    8 │ 0.817685
  ⋮   │    ⋮
 1994 │ 0.805926
 1995 │ 0.805926
 1996 │ 0.805926
 1997 │ 0.805926
 1998 │ 0.805926
 1999 │ 0.805926
 2000 │ 0.805926
</span><span class="sgr36">1985 rows omitted, 0.72)</span></span></code></pre><pre><code class="language-julia hljs">histogram(mh_samples.p, bin = 100, label = &quot;MH&quot;; xlabel=&quot;p&quot;)</code></pre><img src="76d056ad.svg" alt="Example block output"/><h3 id="Transitional-Markov-Chain-Monte-Carlo"><a class="docs-heading-anchor" href="#Transitional-Markov-Chain-Monte-Carlo">Transitional Markov Chain Monte Carlo</a><a id="Transitional-Markov-Chain-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Transitional-Markov-Chain-Monte-Carlo" title="Permalink"></a></h3></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../metamodels/">« Metamodelling</a><a class="docs-footer-nextpage" href="../hpc/">High Performance Computing »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Tuesday 25 June 2024 16:32">Tuesday 25 June 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
