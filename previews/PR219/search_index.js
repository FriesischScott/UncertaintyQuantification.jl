var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"E. Nikolaidis. Types of Uncertainty in Design Decision Making. In: Engineering Design Reliability Handbook, edited by  (CRC Press, 2004).\n\n\n\nJ. M. Aughenbaugh and C. J. Paredis. The Value of Using Imprecise Probabilities in Engineering Design. Journal of Mechanical Design 128, 969–979 (2005).\n\n\n\nA. D. Kiureghian and O. Ditlevsen. Aleatory or Epistemic? Does It Matter? Structural Safety 31, 105–112 (2009).\n\n\n\nS. Bi, M. Broggi, P. Wei and M. Beer. The Bhattacharyya Distance: Enriching the P-box in Stochastic Sensitivity Analysis. Mechanical Systems and Signal Processing 129, 265–281 (2019).\n\n\n\nS. Ferson, V. Kreinovick, L. Ginzburg, F. Sentz and D. Meyers. Constructing Probability Boxes and Dempster-Shafer Structures. Technical Report SAND2015-4166J (Sandia National Laboratory, 2015).\n\n\n\nM. Besançon, T. Papamarkou, D. Anthoff, A. Arslan, S. Byrne, D. Lin and J. Pearson. Distributions.jl: Definition and Modeling of Probability Distributions in the JuliaStats Ecosystem. Journal of Statistical Software 98, 1–30 (2021).\n\n\n\nM. Sklar. Fonctions de repartition a n dimensions et leurs marges. Publ. inst. statist. univ. Paris 8, 229–231 (1959).\n\n\n\nH. Joe. Dependence Modeling with Copulas (CRC Press, 2015).\n\n\n\nC. Dang, M. A. Valdebenito, J. Song, P. Wei and M. Beer. Estimation of Small Failure Probabilities by Partially Bayesian Active Learning Line Sampling: Theory and Algorithm. Computer Methods in Applied Mechanics and Engineering 412, 116068 (2023).\n\n\n\nR. Rackwitz and B. Flessler. Structural Reliability under Combined Random Load Sequences. Computers & Structures 9, 489–494 (1978). Accessed on Oct 26, 2023.\n\n\n\nR. Melchers. Importance Sampling in Structural Systems. Structural Safety 6, 3–10 (1989).\n\n\n\nP. Koutsourelakis, H. Pradlwarter and G. Schuëller. Reliability of Structures in High Dimensions, Part I: Algorithms and Applications. Probabilistic Engineering Mechanics 19, 409–417 (2004).\n\n\n\nM. De Angelis, E. Patelli and M. Beer. Advanced Line Sampling for Efficient Robust Reliability Analysis. Structural Safety 52, 170–182 (2015).\n\n\n\nS.-K. Au and J. L. Beck. Estimation of Small Failure Probabilities in High Dimensions by Subset Simulation. Probabilistic Engineering Mechanics 16, 263–277 (2001).\n\n\n\nK. Zuev. Subset Simulation Method for Rare Event Estimation: An Introduction. In: Encyclopedia of Earthquake Engineering (Springer, Berlin, Heidelberg, 2013).\n\n\n\nS.-K. Au and E. Patelli. Rare Event Simulation in Finite-Infinite Dimensional Space. Reliability Engineering & System Safety 148, 67–77 (2016).\n\n\n\nN. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller and E. Teller. Equation of State Calculations by Fast Computing Machines. The Journal of Chemical Physics 21, 1087–1092 (1953).\n\n\n\nW. K. Hastings. Monte Carlo Sampling Methods Using Markov Chains and Their Applications. Biometrika 57, 97–109 (1970).\n\n\n\nH. Raiffa and R. Schaifer. Applied Statistical Decision Theory. Studies in Managerial Economics (Division of Research, Graduate School of Business Adminitration, Harvard University, 1961).\n\n\n\nJ. Ching and Y.-C. Chen. Transitional Markov Chain Monte Carlo Method for Bayesian Model Updating, Model Class Selection, and Model Averaging. Journal of Engineering Mechanics 133, 816–832 (2007).\n\n\n\nE. Patelli and S. K. Au. Efficient Monte Carlo Algorithm for Rare Failure Event Simulation. In: 12th International Conference on Applications of Statistics and Probability in Civil Engineering, ICASP 2012 (University of British Columbia, Jul 2015).\n\n\n\n","category":"page"},{"location":"examples/hpc/#High-Performance-Computing","page":"High Performance Computing","title":"High Performance Computing","text":"","category":"section"},{"location":"examples/hpc/#OpenMC-TBR-uncertainty","page":"High Performance Computing","title":"OpenMC TBR uncertainty","text":"","category":"section"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"In this example, we will run OpenMC, to compute the tritium breeding ratio (TBR) uncertainty, by varying material and geometric properties. This example was taken from the Fusion Neutronics Workshop.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"At first we need to create an array of random variables, that will be used when evaluating the points that our design produces.It will also define the range of the function we want the design to fit. This is also a good time to declare the function that we are working with.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Here we will vary the model's Li6 enrichment, and the radius of the tungsten layer.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"using UncertaintyQuantification\n\nE = RandomVariable(Uniform(0.3, 0.7), :E)\nR1 = RandomVariable(Uniform(8, 14), :R1)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Source/Extra files are expected to be in this folder.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"sourcedir = joinpath(pwd(), \"demo/models\")","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"These files will be rendered through Mustache.jl and have values injected.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"sourcefile = \"openmc_TBR.py\"","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Dictionary to map format Strings (Format.jl) to variables.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"numberformats = Dict(:E => \".8e\")","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"UQ will create subfolders in here to run the solver and store the results.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"workdir = joinpath(pwd(), \"openmc_TBR\")","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"note: Note\nIf Slurm is to run the jobs on multiple nodes, all the above folders must be shared by the computing nodes.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Read output file and compute the Tritium breeding ratio. An extractor is based the working directory for the current sample.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"TBR = Extractor(base -> begin\n    file = joinpath(base, \"openmc.out\")\n    line = readline(file)\n    tbr = parse(Float64, split(line, \" \")[1])\n\n    return tbr\nend, :TBR)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"In this example, an OpenMC model is built and run using the Python API. We therefore specify the python3` command, and the python file to run.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"openmc = Solver(\n    \"python3\", # path to python3 binary\n    \"openmc_TBR.py\";\n    args=\"\", # (optional) extra arguments passed to the solver\n)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"slurm  sbatch options","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"options = Dict(\n    \"job-name\" => \"UQ_slurm\",\n    \"account\" => \"EXAMPLE-0001-CPU\",\n    \"partition\" => \"cpu_partition\",\n    \"nodes\" => \"1\",\n    \"ntasks\" => \"1\",\n    \"time\" => \"00:05:00\",\n)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Slurm interface, passing required machine information. Note in extras, we specify the commands we require to run the model (for example, loading modulefiles or data).","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"slurm = SlurmInterface(\n    options;\n    throttle=50,\n    extras=[\"module load openmc\", \"source ~/.virtualenvs/openmc/bin/activate\"],\n)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"With the SlurmInterface defined we can assemble the ExternalModel.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"ext = ExternalModel(\n    sourcedir,\n    sourcefile,\n    TBR,\n    openmc;\n    workdir=workdir,\n    formats=numberformats,\n    scheduler=slurm,\n)","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Specify a limitstate function, negative value consititutes failure. Here we are interested in P(TBR <= 1).","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"function limitstate(df)\n    return reduce(vcat, df.TBR) .- 1\nend","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Finally, we can run a Monte Carlo simulation to obtain the probability of failure.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"@time pf, σ, samples = probability_of_failure(ext, limitstate, [E, R1], MonteCarlo(5000))","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"The simulation results in pf = 0.0064 and σ = 0.001127 for 5000 samples. Also TBR mean = 1.2404114576444423, TBR std = 0.10000460056126671, and TBR 95%: [1.0379178446904211, 1.4130216792418262].","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"We can also obtain the probability of failure with Subset simulation.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"subset = SubSetInfinity(800, 0.1, 10, 0.5)\n\n@time pf, std, samples = probability_of_failure(ext, limitstate, [E, R1], subset)\nprintln(\"Probability of failure: $pf\")","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"This results in pf = 0.0053 and σ = 0.001133 for 2400 samples.","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"","category":"page"},{"location":"examples/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"This page was generated using Literate.jl.","category":"page"},{"location":"manual/metamodels/#Metamodels","page":"Metamodelling","title":"Metamodels","text":"","category":"section"},{"location":"manual/metamodels/#Design-Of-Experiments","page":"Metamodelling","title":"Design Of Experiments","text":"","category":"section"},{"location":"manual/metamodels/","page":"Metamodelling","title":"Metamodelling","text":"Design Of Experiments (DOE) offers various designs that can be used for creating a model of a given system. The core idea is to evaluate significant points of the system in order to obtain a sufficient model while keeping the effort to achieve this relatively low. Depending on the parameters, their individual importance and interconnections, different designs may be adequate.","category":"page"},{"location":"manual/metamodels/","page":"Metamodelling","title":"Metamodelling","text":"The ones implemented here are TwoLevelFactorial, FullFactorial, FractionalFactorial, CentralComposite, BoxBehnken and PlackettBurman.","category":"page"},{"location":"manual/metamodels/#Response-Surface","page":"Metamodelling","title":"Response Surface","text":"","category":"section"},{"location":"manual/metamodels/","page":"Metamodelling","title":"Metamodelling","text":"A Response Surface is a simple polynomial surrogate model. It can be trained by providing it with evaluated points of a function or any of the aforementioned experimental designs.","category":"page"},{"location":"examples/bayesianupdating/#Bayesian-Updating","page":"Bayesian Updating","title":"Bayesian Updating","text":"","category":"section"},{"location":"examples/bayesianupdating/#Inverse-eigenvalue-problem","page":"Bayesian Updating","title":"Inverse eigenvalue problem","text":"","category":"section"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The inverse eigenvalue problem is a classic engineering example. Here we will use Bayesian updating to sample from a bivariate posterior distribution describing unknown quantities of a matrix","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"beginbmatrix\n theta_1 + theta_2  -theta_2 \n -theta_2  theta_2 \nendbmatrix","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"A matrix of this form can represent different problems, like the stiffness matrix describing a tuned mass damper system. In this example we assume the fixed values theta_1 = 0.5 and theta_2 = 1.5 for the variables.","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The eigenvalues lambda_1 and lambda_2 of this matrix represent a physical measurable property corrupted by \"noise\" created for example due to environmental factors or measurement inaccuracy.","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"lambda_1^noisy = frac(theta_1+2theta_2)+sqrttheta_1^2+4theta_2^22 + epsilon_1","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"lambda_2^noisy = frac(theta_1+2theta_2)-sqrttheta_1^2+4theta_2^22 + epsilon_2","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The \"noise\" terms epsilon_1 and epsilon_2 follow a Normal distribution with zero mean and standard deviations sigma_1 = 10 and sigma_2 = 01.","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The synthetic \"noisy\" data used for the Bayesian updating procedure is given in the following table.","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"lambda_1 lambda_2\n1.51 0.33\n4.01 0.30\n3.16 0.17\n3.21 0.18\n2.19 0.32\n1.71 0.23\n2.73 0.21\n5.51 0.20\n1.95 0.11\n4.48 0.20\n1.43 0.16\n2.91 0.26\n3.91 0.23\n3.58 0.25\n2.62 0.25","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The a priori knowledge of theta_1 and theta_2 is that they take values between 0.01 and 4. The likelihood function used for this problem is a bivariate Gaussian function with a covariance matrix beginbmatrix sigma_1^2  0  0  sigma_2^2 endbmatrix, with off-diagonal terms equal to 0 and the diagonal terms corresponding to the variances of the respective noise terms.","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"P(lambdatheta) propto exp left-frac12sum_i=1^2sum_n=1^15 left(fraclambda_in^data-lambda_i^modelsigma_iright)^2right","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"To begin the Bayesian model updating procedure we start by defining the data, the models for the eigenvalues (without the noise term) and the likelihood function.","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"using UncertaintyQuantification # hide\nusing Plots # hide\n\n\nY = [\n    1.51 0.33\n    4.01 0.3\n    3.16 0.27\n    3.21 0.18\n    2.19 0.33\n    1.71 0.23\n    2.73 0.21\n    5.51 0.2\n    1.95 0.11\n    4.48 0.2\n    1.43 0.16\n    2.91 0.26\n    3.81 0.23\n    3.58 0.25\n    2.62 0.25\n]\n\nλ1 = @. Model(df -> ((df.θ1 + 2 * df.θ2) + sqrt(df.θ1^2 + 4 * df.θ2^2)) / 2, :λ1)\nλ2 = @. Model(df -> ((df.θ1 + 2 * df.θ2) - sqrt(df.θ1^2 + 4 * df.θ2^2)) / 2, :λ2)\n\nσ = [1.0 0.1]\nfunction likelihood(df)\n    λ = [df.λ1 df.λ2]\n\n    return log.(exp.([-0.5 * sum(((Y .- λ[n, :]') ./ σ) .^ 2) for n in axes(λ, 1)]))\nend","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"We will solve this problem using the TMCMC algorithm. Therefore, the next step is to define the RandomVariable vector of the prior, followed by the TransitionalMarkovChainMonteCarlo object.","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"prior = RandomVariable.(Uniform(0.01, 4), [:θ1, :θ2])\n\nn = 1000\nburnin = 0\n\ntmcmc = TransitionalMarkovChainMonteCarlo(prior, n, burnin)","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"With the prior, likelihood, models and  MCMC sampler defined, the last step is to call the bayesinupdating method.","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"samples, evidence = bayesianupdating(likelihood, [λ1, λ2], tmcmc)\n\nscatter(samples.θ1, samples.θ2; lim=[0, 4], label=\"TMCMC\", xlabel=\"θ1\", ylabel=\"θ2\")","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"A scatter plot of the resulting samples shows convergence to two distinct regions. Unlike the transitional Markov Chain Monte Carlo algorithm, the standard Metropolis-Hastings algorithm would have only identified one of the two regions.","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"","category":"page"},{"location":"examples/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"This page was generated using Literate.jl.","category":"page"},{"location":"benchmarks/subset/#High-dimensional-Subset-simulation","page":"Subset Simulation","title":"High dimensional Subset simulation","text":"","category":"section"},{"location":"benchmarks/subset/#Subset-simulation","page":"Subset Simulation","title":"Subset simulation","text":"","category":"section"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"The implemented subset simulation algorithms SubSetSimulation (using Metropolis-Hastings MCMC), SubSetInfinity (conditional sampling MCMC) and SubSetInfinityAdaptive (adaptive conditional sampling MCMC), work efficiently in high dimensions. This benchmark shows how these algorithms scale with increasing number of dimension N and increasingly smaller target probability of failure pf_target.","category":"page"},{"location":"benchmarks/subset/#Example-function","page":"Subset Simulation","title":"Example function","text":"","category":"section"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"In this example, the test model will be sum of independent standard normal distributions","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"f_N(X) = sum^N_i X_i","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"where X_i sim Phi(0 1) are standard normal random variables. We will define a linear limitstate","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"g_N(X) = C_N - f_N(X)","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"where C_N will be defined such that the failure probability mathbbP(g(X) leq 0) matches a pre-defined value pf_target.","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"We can find C_N analytically, depending on the chosen number of dimensions and target probability of failure","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"C_N = F_Phi_sqrtN^-1(1 - p_texttarget)","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"where F_Phi_sqrtN^-1 is the quantile function of a Gaussian distribution, with zero mean and standard deviation sqrt(N).","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"Since the dimension and failure probability are two parameters of this numerical experiment, we can dynamically create the required number of random variables using broadcasting","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"using UncertaintyQuantification\n\nN = 2000\n\ninputs = RandomVariable.(Normal(), [Symbol(\"x$i\") for i in 1:N])\n","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"The model can be defined generalized for arbitrary dimensions by summing the columns of the DataFrame. Using names(inputs) to select the columns we can safely exclude any extra variables that might be present.","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"f = Model(\n    df -> sum(eachcol(df[:, names(inputs)])),\n    :f\n)","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"Next, the pf_target and corresponding limit state are defined.","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"pf_target = 1e-9\n\nfail_limit = quantile(Normal(0, sqrt(N)), 1 - pf_target)\n\nfunction g(df)\n    return fail_limit .- df.f\nend","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"For this benchmark, the probability of failure will be estimated using all available variants of Subset simulation","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"subset_MH = SubSetSimulation(2000, 0.1, 20, Uniform(-0.5, 0.5))\nsubset_CS = SubSetInfinity(2000, 0.1, 20, 0.5)\nsubset_aCS = SubSetInfinityAdaptive(2000, 0.1, 20, 200)","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"note: Monte Carlo simulation\nAlthough standard Monte Carlo simulation works independently of dimension, for a target failure probability of 10^-9, even with a billion 10^9 samples it can return p_f=0.","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"As a first benchmark the three subset simulation algorithms are used to solve the example problem with a fixed number of dimensions N=200 and sample size per level N_samples=2000 for increasingly smaller target probability of failures. The following figure shows the average estimated probabilities of failure and the standard deviation resulting from 100 independent simulation runs. Note how the variance of the estimation increases for smaller pf values. However, in comparison, the variance of the Metropolis-Hastings variant is higher than the variance of the conditional sampling methods.","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"(Image: Subset simulation for smaller pfs)","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"For the next benchmark, the number of dimensions remains fixed at 200 while the  number of samples is increased to estimate a target probability of failure of 10^-4.","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"(Image: Subset simulation with increasing samples)","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"The final benchmark uses the same fixed target p_f = 10^-4 and again keeps the number of samples constant at 2000 per level. This time, the number of dimensions is increased to raise the complexity of the problem.","category":"page"},{"location":"benchmarks/subset/","page":"Subset Simulation","title":"Subset Simulation","text":"(Image: Subset simulation with increasing dimensions)","category":"page"},{"location":"manual/reliability/#Reliability-Analysis","page":"Reliability Analysis","title":"Reliability Analysis","text":"","category":"section"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"In the context of structural engineering and risk assessment, the term reliability is used to describe the ability of system to perform its intended function under varying conditions over time.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"There, the performance of a system is identified by its performance function g(boldsymbolx) such that:","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"g(\\boldsymbol{x}) = \\begin{cases}     > 0 & \\text{safe\\ domain}\\\n    \\leq 0 & \\text{failure \\ domain}\\\n\\end{cases}. $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Then the probability of failure is defined as the likelihood of the system being in the failed state, given as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"pf = \\int{g(\\boldsymbol{x}) \\leq 0} f_{\\boldsymbol{X}}(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}. $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Here, f_boldsymbolX(boldsymbolx) denotes the joint probability density function (PDF) of the input boldsymbolX.","category":"page"},{"location":"manual/reliability/#Definition-of-the-Input,-Model-and-Performance","page":"Reliability Analysis","title":"Definition of the Input, Model and Performance","text":"","category":"section"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"The first step of the implementation of a reliability analysis in UncertaintyQuantification.jl is the definition of the probabilistic input and the model which is shown exemplarily.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Using an example from [9] with a probabilistic input boldsymbolX = X_1 X_2 which are both standard normal random variables and a model","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"y(\\boldsymbol{X}) = X2 + 0.01 X1^3 + \\sin(X_1). $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Using that model, the performance function is defined as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"g(\\boldsymbol{X}) = 5 - y(\\boldsymbol{X}). $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"The probabilistic input containing the two standard normal random variables is implemented as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"using UncertaintyQuantification, DataFrames # hide\nusing Random # hide\nx = RandomVariable.(Normal(), [:x1, :x2])","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Next we define the model for the response y(boldsymbolX) as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"y = Model(df -> df.x2 .+ 0.01 * df.x1.^3 .+ sin.(df.x1), :y)\nnothing # hide","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"where the first input is the function y (which must accept a DataFrame) and the second argument is the Symbol for the output variable. With the help of the model, we can define the performance function g which again takes a DataFrame as an input:","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"g(df) = 5 .- df.y\nnothing # hide","category":"page"},{"location":"manual/reliability/#Approximation-Methods","page":"Reliability Analysis","title":"Approximation Methods","text":"","category":"section"},{"location":"manual/reliability/#First-Order-Reliability-Method","page":"Reliability Analysis","title":"First Order Reliability Method","text":"","category":"section"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"The First Order Reliability Method (FORM) [10] estimates the failure probability by finding a linear approximation of the performance function at the so-called design point boldsymbolU^*. The design point represents the point on the surface of the performance function g(boldsymbolX) = 0 that is closest to the origin in the standard normal space.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"That distance from the design point to the origin is referred to as the reliability index given as beta^* = boldsymbolU^*. Due to the transformation to the standard normal space, the probability of failure is simply given as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"\\hat{p}_{f, \\mathrm{FORM}} = \\Phi(-\\beta^*) $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"where Phi() denotes the standard normal CDF.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"In addition to the beta^*, the location of the design point is specified by the important direction defined as:","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"\\boldsymbol{\\alpha}^* = \\frac{\\boldsymbol{U}^}{||\\boldsymbol{U}^||}. $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"In UncertaintyQuantification.jl a FORM analysis can be performed calling probability_of_failure(model, performance, input, simulation) where FORM() is passed as the simulation method:","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"pf_form, β, dp = probability_of_failure(y, g, x, FORM())\n\nprintln(\"Probability of failure: $pf_form\")","category":"page"},{"location":"manual/reliability/#Simulation-Methods","page":"Reliability Analysis","title":"Simulation Methods","text":"","category":"section"},{"location":"manual/reliability/#Monte-Carlo-Simulation","page":"Reliability Analysis","title":"Monte Carlo Simulation","text":"","category":"section"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Monte Carlo Simulation (MCS) offers an approximation of the failure probability using stochastic simulation.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"It utilizes an indicator function of the failure domain","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"\\mathbb{I}[g(\\boldsymbol{x})] = \\begin{cases}     0 & \\text{when} \\ g(\\boldsymbol{x}) > 0\\\n    1 & \\text{when} \\ g(\\boldsymbol{x}) \\leq 0\\\n\\end{cases}. $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"This allows for the failure probability to be interpreted as the expected value of the indicator function","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"pf = \\int{\\boldsymbol{X}} \\mathbb{I}[g(\\boldsymbol{x})]\\ f_{\\boldsymbol{X}}(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x} = \\mathbb{E}\\big[\\mathbb{I}[g(\\boldsymbol{x})]\\big]. $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"The Monte Carlo estimate of the failure probability is given as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"pf \\approx \\hat{p}f = \\frac{1}{N} \\sum{i=1}^N \\mathbb{I}[g(\\boldsymbol{x}i)] $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"where boldsymbolx_i_i=1^N represents a set of N samples drawn from the input PDF f_boldsymbolX(boldsymbolx). The variance of the estimator is given as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"\\operatorname{Var}[\\hat{p}f] = \\frac{\\hat{p}f (1-\\hat{p}_f)}{N}. $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"In UncertaintyQuantification.jl we can perform a Monte Carlo Simulation by defining the analysis as MonteCarlo(n)  where n is the number of samples","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Random.seed!(42) # hide\nmc = MonteCarlo(10^7)\nnothing","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Then the reliability analysis is performed by calling probability_of_failure(model, performance, input, simulation).","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"pf_mc, std_mc, samples = probability_of_failure(y, g, x, mc)\n\nprintln(\"Probability of failure: $pf_mc\")\nprintln(\"Coefficient of variation: $(std_mc/pf_mc)\")","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"A comparison to the result obtained using FORM highlights the limitation imposed by the linear approximation of the limit-state surface.","category":"page"},{"location":"manual/reliability/#Importance-Sampling","page":"Reliability Analysis","title":"Importance Sampling","text":"","category":"section"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Based on the standard MCS method, a class of advanced method exist that have to goal to accelerate the estimation of the failure probability by requiring fewer model calls.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Importance Sampling [11] introduces a second density that is biased in a way that it generates more samples in the failure domain. Typically such a density is constructed around the design point obtained in a preceding FORM analysis.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"In order to perform a reliability analysis using Importance Sampling, we again have to specify the number of samples and then can probability_of_failure().","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"is = ImportanceSampling(1000)\npf_is, std_is, samples = probability_of_failure(y, g, x, is)\n\nprintln(\"Probability of failure: $pf_is\")\nprintln(\"Coefficient of variation: $(std_is/pf_is)\")","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Since Importance Sampling relies on FORM, the results are similar to those of FORM.","category":"page"},{"location":"manual/reliability/#Line-Sampling","page":"Reliability Analysis","title":"Line Sampling","text":"","category":"section"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Another advanced Monte Carlo method for reliability analysis is Line Sampling [12]. Its main idea is to use parallel lines for sampling rather than points.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Therefore first the problem is transformed into the standard normal space to make use of the invariance of rotation. The important direction boldsymbolalpha is determined, e.g., using FORM or the gradient at the origin. Then, samples are generated and projected onto the hyperplane orthogonal to boldsymbolalpha. From each point on the hyperplane, a line is drawn parallel to boldsymbolalpha and its intersection with the performance function is determined using root finding based on a spline interpolation scheme, giving the set of distances beta^(i)_i=1^N from the hyperplane to the intersection with the performance function. Due to working in the standard normal space, the failure probability along each line is given as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"p_{f, \\mathrm{line}}^{(i)} = \\Phi(-\\beta^{(i)}) $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Finally, the probability of failure is obtained as the mean of the failure probabilities along the lines","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"\\hat{p}{f,\\mathrm{LS}} = \\frac{1}{N} \\sum{i=1}^N p_{f, \\mathrm{line}}^{(i)}. $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"The variance of hatp_fmathrmLS is given by the variance of the line failure probabilities:","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"\\operatorname{Var}[\\hat{p}{f,\\mathrm{LS}}] = \\frac{1}{N(N-1)} \\sum{i=1}^N \\Big(p{f, \\mathrm{line}}^{(i)} - \\hat{p}{f,\\mathrm{LS}}\\Big)^2. $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Similar to standard MCS, we have to pass N to the Line Sampling method. However, here we pass the number of lines. Optionally, we can pass a vector of the points along each line that are used to evaluate the performance function and a per-determined direction boldsymbolalpha:","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"ls = LineSampling(200, collect(0.5:0.5:10))\npf_ls, std_ls, samples = probability_of_failure([y], g, x, ls)\n\nprintln(\"Probability of failure: $pf_ls\")\nprintln(\"Coefficient of variation: $(std_ls/pf_ls)\")","category":"page"},{"location":"manual/reliability/#Advanced-Line-Sampling","page":"Reliability Analysis","title":"Advanced Line Sampling","text":"","category":"section"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Advanced Line Sampling [13] is a further enhancement of the standard line sampling methods due to two main features:","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"The important direction boldsymbolalpha is adapted once a more probable point is found\nThe lines are processed sorted by proximity of the points on the hyperplane.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Especially the second point enables the use of an iterative root finder using Newton's method.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"The definition of the AdvancedLineSampling simulation method is similar to that of regular Line Sampling. The number of lines has to be given to the constructor and we can optionally give the number of points along the line which is only used to find the starting point of the iterative root search.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"als = AdvancedLineSampling(200, collect(0.5:0.5:10))\npf_als, std_als, samples = probability_of_failure([y], g, x, als)\n\nprintln(\"Probability of failure: $pf_als\")\nprintln(\"Coefficient of variation: $(std_als/pf_als)\")","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"For AdvancedLineSampling, we can also define the (initial) direction and options of the iterative root finding, i.e., the tolerance, stepsize of the gradient and maxiterations.","category":"page"},{"location":"manual/reliability/#Subset-Simulation","page":"Reliability Analysis","title":"Subset Simulation","text":"","category":"section"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Subset simulation [14] is an advanced simulation technique for the estimation of small failure probabilities. This approach involves decomposing the problem into a sequence of conditional probabilities that are estimated using Markov Chain Monte Carlo.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Here we solve a simple problem taken from [15] where the response y(boldsymbolX) depends on two independent random variables X_1 and X_2 following a standard normal distribution. The simple linear model is defined by Subset simulation [14] is an advanced simulation technique for the estimation of small failure probabilities. This approach involves decomposing the problem into a sequence of conditional probabilities that are estimated using Markov Chain Monte Carlo.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Here we solve a simple problem taken from [15] where the response y(boldsymbolX) depends on two independent random variables X_1 and X_2 following a standard normal distribution. The simple linear model is defined by","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"y(\\boldsymbol{X})= X1 + X2 $ $ y(\\boldsymbol{X})= X1 + X2 $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"with the performance function with the performance function","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"g(\\boldsymbol{X}) = 9 - y(\\boldsymbol{X}). $ $ g(\\boldsymbol{X}) = 9 - y(\\boldsymbol{X}). $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"The analytical probability of failure can be calculated as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"$","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"pf = 1 - \\Phi\\Bigg(\\frac{9}{\\sqrt(2)}\\Bigg) \\approx 1 \\times 10^{-10}. $ $ pf = 1 - \\Phi\\Bigg(\\frac{9}{\\sqrt(2)}\\Bigg) \\approx 1 \\times 10^{-10}. $","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"In order to solve this, we start by creating the two random variables and group them in a vector inputs.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"using UncertaintyQuantification, DataFrames # hide\nusing Random; Random.seed!(8128) # hide\nx1 = RandomVariable(Normal(), :x1)\nx2 = RandomVariable(Normal(), :x2)\ninputs = [x1, x2]","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Next we define the model as","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"y = Model(df -> df.x1 + df.x2, :y)\nnothing # hide","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"where the first input is our function (which must accept a DataFrame) and the second the Symbol for the output variable.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"To estimate a failure probability we need a performance which is negative if a failure occurs.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"function g(df::DataFrame)\n    return 9 .- df.y\nend\nnothing # hide","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Finally, we create the SubSetSimulation object and compute the probability of failure using a standard Gaussian proposal PDF. The value for the target probability of failure at each intermediate level is set to 01 which is generally accepted as the optimal value.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"subset = SubSetSimulation(1000, 0.1, 10, Normal())\npf, std, samples = probability_of_failure(y, g, inputs, subset)\n\nprintln(\"Probability of failure: $pf\")","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"Alternatively, instead of using the standard Subset simulation algorithm (which internally uses Markov Chain Monte Carlo), we can use SubSetInfinity to compute the probability of failure, see [16]. Here we use a standard deviation of 05 to create the proposal samples for the next level.","category":"page"},{"location":"manual/reliability/","page":"Reliability Analysis","title":"Reliability Analysis","text":"subset = SubSetInfinity(1000, 0.1, 10, 0.5)\npf, std, samples = probability_of_failure(y, g, inputs, subset)\n\nprintln(\"Probability of failure: $pf\")","category":"page"},{"location":"manual/introduction/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"manual/introduction/#Uncertainty","page":"Introduction","title":"Uncertainty","text":"","category":"section"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"The definition of uncertainty follows from the absence of certainty describing a state of absolute knowledge where everything there is to know about a process is known [1]. This however is a theoretical and unachievable state in which deterministic models would be sufficient for the analysis of engineering systems. In reality, there is always a gap between certainty and the current state of knowledge resulting in uncertainty.","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"(Image: Characterization of uncertainties)\nCharacterization of reducible and irreducible uncertainties. Adapted from [2].","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"Although a topic of ongoing debate and revision [2], it is largely accepted that uncertainty can be broadly classified into two types, aleatory and epistemic uncertainty [3]. The first type, aleatory uncertainties, are also called irreducible uncertainties or variability and describe the inherent randomness of a process. This could, for example, be variability in material properties, degradation of components, or varying external forces such as wind loads or earthquakes. Some researchers debate the existence of aleatory uncertainty under the assumption that if a process was fully understood it would no longer be random. Epistemic uncertainty is the uncertainty resulting from a lack of knowledge or vagueness and is also called reducible uncertainty, as it can be reduced through the collection of additional data and information. If both types of uncertainties occur together this is sometimes called hybrid or mixed uncertainty, and can be modelled using imprecise probability.","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"We follow Bi et al. [4] in dividing uncertainties into four categories:     - Category I: Constant parameters without any associated uncertainty,     - Category II: Parameters only subject to epistemic uncertainty represented as intervals,     - Category III: Variables with only aleatory uncertainties, fully described by probability distributions,     - Category IV: Variables subject to both aleatory and epistemic uncertainty represented using imprecise probabilities, for example using probability boxes.","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"Next follows a brief introduction to the modelling of precise (category III) and imprecise probabilities (category IV).","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"A continuous random variable X is uniquely defined by its cumulative density function (CDF) F mathbbR rightarrow 01. By definition, it returns the probability that the random variable will take a value less than or equal to x","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"    F_X(x) = P_X(X leq x)","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"A CDF is a right continuous and monotonically non-decreasing function which satisfies","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"    lim_xrightarrow -infty F_X(x) = 0","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"and","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"    lim_xrightarrow infty  F_X(x) = 1","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"The probability density function (PDF) of a random variable can be obtained as the derivative of the CDF","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"    f_X(x) = fracdF_X(x)dx","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"if it exists. Conversely, the CDF can be defined as the integral of the PDF as","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"    F_X(x) = int_-infty^x f_X(lambda) dlambda","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"Using the PDF and CDF random variables subject to aleatory uncertainty can be described using well established probability theory.","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"Ferson et al. [5] introduced the notion of a probability box (p-box) for the representing variables witg both epistemic and aleatory uncertainty. Consider two CDFs underlineF and overlineF with underlineF(x) leq overlineF(x) for all x in mathbbR  Then underlineF(x) overlineF(x) is the set of CDFs F such that underlineF(x) leq F(x) leq overlineF(x). This set is called the p-box for an imprecisely known random variable X, where underlineF(x) is the lower bound for the probability that X is smaller than or equal to x, and overlineF(x) is the upper bound of this probability.","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"The simplest way to construct a p-box is using a known parametric family (normal, exponential, ...) with intervals for their parameters (mean, variance, ...), and from this we can form the set underlineF(x) overlineF(x). This is known as parametric p-box, only containing distributions following the specified distribution family. If no family information is available, but underlineF(x) overlineF(x) are known, called a distribution-free p-box, where every possible CDF between the bounds is a valid random variable.","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"Special algorithms must be used to propagate the epistemic uncertainty through models. As a result, the analysis also returns upper and lower bounds. This propagation of the epistemic uncertainty comes with a significant increase in computational demand, requiring specialised algorithms or perhaps surrogate modelling.","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"In UncertaintyQuantification.jl the four categories of uncertainties are described using the following objects:","category":"page"},{"location":"manual/introduction/","page":"Introduction","title":"Introduction","text":"Category I: Parameter\nCategory II: Interval\nCategory III: RandomVariable\nCategory IV: ProbabilityBox.","category":"page"},{"location":"api/bayesianupdating/#Bayesian-Updating","page":"Bayesian Updating","title":"Bayesian Updating","text":"","category":"section"},{"location":"api/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Markov Chain Monte Carlo Methods for Bayesian updating.","category":"page"},{"location":"api/bayesianupdating/#Index","page":"Bayesian Updating","title":"Index","text":"","category":"section"},{"location":"api/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Pages = [\"bayesianupdating.md\"]","category":"page"},{"location":"api/bayesianupdating/#Types","page":"Bayesian Updating","title":"Types","text":"","category":"section"},{"location":"api/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"AbstractBayesianMethod\nSingleComponentMetropolisHastings\nTransitionalMarkovChainMonteCarlo","category":"page"},{"location":"api/bayesianupdating/#UncertaintyQuantification.AbstractBayesianMethod","page":"Bayesian Updating","title":"UncertaintyQuantification.AbstractBayesianMethod","text":"AbstractBayesianMethod\n\nSubtypes are used to dispatch to the differenct MCMC methods in bayesianupdating.\n\nSubtypes are:\n\nSingleComponentMetropolisHastings\nTransitionalMarkovChainMonteCarlo\n\n\n\n\n\n","category":"type"},{"location":"api/bayesianupdating/#UncertaintyQuantification.SingleComponentMetropolisHastings","page":"Bayesian Updating","title":"UncertaintyQuantification.SingleComponentMetropolisHastings","text":"SingleComponentMetropolisHastings(proposal, x0, n, burnin, islog)\n\nPassed to bayesianupdating to run the single-component Metropolis-Hastings algorithm starting from x0 with  univariate proposal distibution proposal. Will generate n samples after performing burnin steps of the Markov chain and discarding the samples. The flag islog specifies whether the prior and likelihood functions passed to the  bayesianupdating method are already  given as logarithms.\n\nAlternative constructor\n\n    SingleComponentMetropolisHastings(proposal, x0, n, burnin)  # `islog` = true\n\n\n\n\n\n","category":"type"},{"location":"api/bayesianupdating/#UncertaintyQuantification.TransitionalMarkovChainMonteCarlo","page":"Bayesian Updating","title":"UncertaintyQuantification.TransitionalMarkovChainMonteCarlo","text":"TransitionalMarkovChainMonteCarlo(prior, n, burnin, β, islog)\n\nPassed to [`bayesianupdating`](@ref) to run thetransitional Markov chain Monte Carlo algorithm  with [`RandomVariable'](@ref) vector `prior`. At each transitional level, one sample will be generated from `n` independent Markov chains after `burnin` steps have been discarded. The flag `islog` specifies whether the prior and likelihood functions passed to the  [`bayesianupdating`](@ref) method are already  given as logarithms.\n\nAlternative constructors\n\n    TransitionalMarkovChainMonteCarlo(prior, n, burnin, β)  # `islog` = true\n     TransitionalMarkovChainMonteCarlo(prior, n, burnin)    # `β` = 0.2,  `islog` = true\n\nReferences\n\n[20]\n\n\n\n\n\n","category":"type"},{"location":"api/bayesianupdating/#Methods","page":"Bayesian Updating","title":"Methods","text":"","category":"section"},{"location":"api/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"bayesianupdating","category":"page"},{"location":"api/bayesianupdating/#UncertaintyQuantification.bayesianupdating","page":"Bayesian Updating","title":"UncertaintyQuantification.bayesianupdating","text":"bayesianupdating(prior, likelihood, models, mcmc)\n\nPerform bayesian updating using the given prior, likelihood, models  and any MCMC sampler AbstractBayesianMethod.\n\nAlternatively the method can be called without models.\n\nbayesianupdating(prior, likelihood, mcmc)\n\nWhen using TransitionalMarkovChainMonteCarlo the prior can automatically be constructed.\n\nbayesinupdating(likelihood, models, tmcmc)\nbayesianupdating(likelihood, tmcmc)\n\nNotes\n\nlikelihood is a Julia function which must be defined in terms of a DataFrame of samples, and must evaluate the likelihood for each row of the DataFrame\n\nFor example, a loglikelihood based on normal distribution using 'Data':\n\nlikelihood(df) = [sum(logpdf.(Normal.(df_i.x, 1), Data)) for df_i in eachrow(df)]\n\nIf a model evaluation is required to evaluate the likelihood, a vector of UQModels must be passed to bayesianupdating. For example if the variable x above is the output of a numerical model.\n\n\n\n\n\n","category":"function"},{"location":"examples/metamodels/#Metamodels","page":"Metamodels","title":"Metamodels","text":"","category":"section"},{"location":"examples/metamodels/#Design-Of-Experiments","page":"Metamodels","title":"Design Of Experiments","text":"","category":"section"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"Design Of Experiments (DOE) offers various designs that can be used for creating a model of a given system. The core idea is to evaluate significant points of the system in order to obtain a sufficient model while keeping the effort to achieve this relatively low. Depending on the parameters, their individual importance and interconnections, different designs may be adequate.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"The ones implemented here are TwoLevelFactorial, FullFactorial, FractionalFactorial, CentralComposite and BoxBehnken.","category":"page"},{"location":"examples/metamodels/#Response-Surface","page":"Metamodels","title":"Response Surface","text":"","category":"section"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"A Response Surface is a structure used for modeling.     It can be trained by providing it with evaluated points of a function.     It will then, using polynomial regression, compute a model of that function.","category":"page"},{"location":"examples/metamodels/#Example","page":"Metamodels","title":"Example","text":"","category":"section"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"In this example, we will model the following test function (known as Himmelblau's function) newlinein the range x1 x2  -5 5. It is defined as newline","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"f(x1 x2) = (x1^2 + x2 - 11)^2 + (x1 + x2^2 - 7)^2","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":".","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"using Plots #hide\na = range(-5, 5; length=1000)   #hide\nb = range(5, -5; length=1000)   #hide\nhimmelblau_f(x1, x2) = (x1^2 + x2 - 11)^2 + (x1 + x2^2 - 7)^2 #hide\ns1 = surface(a, b, himmelblau_f; plot_title=\"Himmelblau's function\")   #hide","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"At first we need to create an array of random variables, that will be used when evaluating the points that our desgin produces. It will also define the range of the function we want the design to fit. This is also a good time to declare the function that we are working with.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"using UncertaintyQuantification\n\nx = RandomVariable.(Uniform(-5, 5), [:x1, :x2])\n\nhimmelblau = Model(\n    df -> (df.x1 .^ 2 .+ df.x2 .- 11) .^ 2 .+ (df.x1 .+ df.x2 .^ 2 .- 7) .^ 2, :y\n)","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"Our next step is to chose the design we want to use and if required, set the parameters to the values we need or want. In this example, we are using a FullFactorial design:","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"design = FullFactorial([5, 5])","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"After that, we call the sample function with our design. This produces a matrix containing the points of our design fitted to the range defined via the RandomVariables. Wer then evaluate the function we want to model in these points and use the resulting data to train a ResponseSurface. The ResponseSurface uses regression to fit a polynomial function to the given datapoints. That functions degree is set as an Integer in the constructor.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"note: Note\nThe choice of the degree and the design and its parameters may be crucial to obtaining a sufficient model.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"training_data = sample(x, design)\nevaluate!(himmelblau, training_data)\nrs = ResponseSurface(training_data, :y, 4)\n\ntest_data = sample(x, 1000)\nevaluate!(rs, test_data)","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"To evaluate the ResponseSurfaceuse evaluate!(rs::ResponseSurface, data::DataFrame) with the dataframe containing the points you want to evaluate.","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"The model in this case has an mse of about 1e-26 and looks like this in comparison to the original:","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"f(x1, x2) = map(m -> m([x1, x2]), rs.monomials') * rs.β #hide\n\ns2 = surface(a, b, f; plot_title=\"Response Surface\", plot_titlefontsize=16) #hide\nsurface(s1, s2; layout=(1, 2), legend=false, size=(800, 400))  #hide","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"","category":"page"},{"location":"examples/metamodels/","page":"Metamodels","title":"Metamodels","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/inputs/#Inputs","page":"Inputs","title":"Inputs","text":"","category":"section"},{"location":"api/inputs/#Index","page":"Inputs","title":"Index","text":"","category":"section"},{"location":"api/inputs/","page":"Inputs","title":"Inputs","text":"Pages = [\"inputs.md\"]","category":"page"},{"location":"api/inputs/#Types","page":"Inputs","title":"Types","text":"","category":"section"},{"location":"api/inputs/","page":"Inputs","title":"Inputs","text":"Parameter\nRandomVariable\nInterval\nProbabilityBox","category":"page"},{"location":"api/inputs/#UncertaintyQuantification.Parameter","page":"Inputs","title":"UncertaintyQuantification.Parameter","text":"Parameter(value::Real, name::Symbol)\n\nDefines a parameter value (scalar), with an input value and a name.\n\nExamples\n\njulia> Parameter(3.14, :π)\nParameter(3.14, :π)\n\n\n\n\n\n","category":"type"},{"location":"api/inputs/#UncertaintyQuantification.RandomVariable","page":"Inputs","title":"UncertaintyQuantification.RandomVariable","text":"RandomVariable(dist::UnivariateDistribution, name::Symbol)\n\nDefines a random variable, with a univariate distribution from Distributions.jl and a name.\n\nExamples\n\njulia> RandomVariable(Normal(), :x)\nRandomVariable(Normal{Float64}(μ=0.0, σ=1.0), :x)\n\njulia> RandomVariable(Exponential(1), :x)\nRandomVariable(Exponential{Float64}(θ=1.0), :x)\n\n\n\n\n\n","category":"type"},{"location":"api/inputs/#UncertaintyQuantification.Interval","page":"Inputs","title":"UncertaintyQuantification.Interval","text":"Interval(lb::Real, up::real, name::Symbol)\n\nDefines an Interval, with lower a bound, an upper bound and a name.\n\nExamples\n\njldoctest julia> Interval(0.10, 0.14, :b) Interval(0.1, 0.14, :b)\n\n\n\n\n\n","category":"type"},{"location":"api/inputs/#UncertaintyQuantification.ProbabilityBox","page":"Inputs","title":"UncertaintyQuantification.ProbabilityBox","text":"ProbabilityBox{T}(p::AbstractVector{Interval}, name::Symbol)\n\nDefines an ProbabilityBox from a Vector of Interval, name UnivariateDistribution T. The number and order of parameters must match the parameters of the associated distribution from Distributions.jl.\n\nExamples\n\njulia>  ProbabilityBox{Uniform}([Interval(1.75, 1.83, :a), Interval(1.77, 1.85, :b)], :l)\nProbabilityBox{Uniform}(Interval[Interval(1.75, 1.83, :a), Interval(1.77, 1.85, :b)], :l, 0.0, 1.0)\n\njulia>  ProbabilityBox{Normal}([Interval(0, 1, :μ), Interval(0.1, 1, :σ)], :x)\nProbabilityBox{Normal}(Interval[Interval(0, 1, :μ), Interval(0.1, 1, :σ)], :x, -Inf, Inf)\n\n\n\n\n\n","category":"type"},{"location":"api/inputs/#Functions","page":"Inputs","title":"Functions","text":"","category":"section"},{"location":"api/inputs/","page":"Inputs","title":"Inputs","text":"sample(rv::RandomVariable, n::Integer)\nsample(inputs::Vector{<:UQInput}, n::Integer)\n","category":"page"},{"location":"api/inputs/#UncertaintyQuantification.sample-Tuple{RandomVariable, Integer}","page":"Inputs","title":"UncertaintyQuantification.sample","text":"sample(rv::RandomVariable, n::Integer)\n\nGenerates n samples from a random variable. Returns a DataFrame.\n\nExamples\n\nSee also: RandomVariable\n\n\n\n\n\n","category":"method"},{"location":"api/inputs/#UncertaintyQuantification.sample-Tuple{Vector{<:UQInput}, Integer}","page":"Inputs","title":"UncertaintyQuantification.sample","text":"sample(inputs::Vector{<:UQInput}, n::Integer)\n\nGenerates n correlated samples from a collection of inputs. Returns a DataFrame\n\nSee also: RandomVariable, Parameter\n\n\n\n\n\n","category":"method"},{"location":"manual/hpc/#High-performance-computing","page":"High Performance Computing","title":"High performance computing","text":"","category":"section"},{"location":"manual/hpc/#Slurm-job-arrays","page":"High Performance Computing","title":"Slurm job arrays","text":"","category":"section"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"When sampling large simulation models, or complicated workflows, Julia's inbuilt parallelism is sometimes insufficient. Job arrays are a useful feature of the slurm scheduler which allow you to run many similar jobs, which differ by an index (for example a sample number). This allows UncertaintyQuantification.jl to run heavier simulations (for example, simulations requiring multiple nodes), by offloading model sampling to an HPC machine using slurm. This way, UncertaintyQuantification.jl can be started on a single worker, and the HPC machine handles the rest.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"For more information on job arrays, see: job arrays.","category":"page"},{"location":"manual/hpc/#SlurmInterface","page":"High Performance Computing","title":"SlurmInterface","text":"","category":"section"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"When SlurmInterface is passed to an ExternalModel, a slurm job array script is automatically generated and executed. Julia waits for this job to finish before extracting results and proceeding.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"using UncertaintyQuantification\n\nslurm = SlurmInterface(;\n    account=\"HPC_account_1\",\n    partition=\"CPU_partition\",\n    jobname=\"UQ_array\",\n    nodes=1,\n    ntasks=32,\n    throttle=50,\n    extras=[\"load python3\"],\n    time=\"01:00:00\",\n)","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Here account is your account (provided by your HPC admin/PI), and partition specifies the queue that jobs will be submitted to (ask admin if unsure). nodes and ntasks are the number of nodes and CPUs that your individual simulations requires. Depending on your HPC machine, each node has a specific number of CPUs. If your application requires more CPUs than are available per node, you can use multiple nodes.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"The parameter time specifies the maximum time that each simulation will be run for, before being killed.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"warning: Individual model runs VS overall batch\nnodes, ntasks, and time are parameters required for each individual model evaluation, not the entire batch. For example, if you are running a large FEM simulation that requires 100 CPUs to evaluate one sample, and your HPC machine has 50 CPUs per node, you would specify nodes = 2 and ntasks = 100.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"note: Compiling with MPI\nIf your model requires multiple nodes, it may be best to compile your application with MPI, if your model allows for it. Please check your application's documentation for compiling with MPI.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Any commands in extras will be executed before you model is run, for example loading any module files or data your model requires. Multiple commands can be passed: extras = [\"load python\", \"python3 get_data.py\"].","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"note: Note\nIf your extras command requires \"\" or $ symbols, they must be properly escaped as \\\"\\\" and \\$.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"The job array task throttle, which is the number of samples that will be run concurrently at any given time, is specified by throttle. For example, when running a MonteCarlo simulation with 2000 samples, and throttle = 50, 2000 model evaluations will be run in total, but only 50 at the same time. If left empty, your scheduler's default throttle will be used.","category":"page"},{"location":"manual/hpc/#Testing-your-HPC-configuration","page":"High Performance Computing","title":"Testing your HPC configuration","text":"","category":"section"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"Slurm is tested only on linux systems, not Mac or Windows. When testing UncertaintyQuantification.jl locally, we use a dummy function test/test_utilities/sbatch to mimic an HPC scheduler.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"warning: Testing locally on Linux\nCertain Slurm tests may fail unless test/test_utilities/ is added to PATH. To do this: export PATH=UncertaintyQuantification.jl/test/test_utilities/:$PATH. Additionally, actual slurm submissions may fail if test/test_utilities/sbatch is called in place of your system installation. To find out which sbatch you're using, call which sbatch.","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"If you'd like to actually test the Slurm interface your HPC machine:","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"using Pkg\nPkg.test(\"UncertaintyQuantification\"; test_args=[\"HPC\", \"YOUR_ACCOUNT\", \"YOUR_PARTITION\"])","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"or if you have a local clone, from the top directory:","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"julia --project\nusing Pkg\nPkg.test(; test_args=[\"HPC\", \"YOUR_ACCOUNT\", \"YOUR_PARTITION\"])","category":"page"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"YOUR_ACCOUNT and YOUR_PARTITION should be replaced with your account and partition you wish to use for testing. This test will submit 4 slurm job arrays, of a lightweight calculation (> 1 minute per job) requiring 1 core/task each.","category":"page"},{"location":"manual/hpc/#Usage","page":"High Performance Computing","title":"Usage","text":"","category":"section"},{"location":"manual/hpc/","page":"High Performance Computing","title":"High Performance Computing","text":"See examples/HPC for a detailed example.","category":"page"},{"location":"manual/bayesianupdating/#Bayesian-Updating","page":"Bayesian Updating","title":"Bayesian Updating","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Bayesian updating is a method of statistical inference where Bayes' theorem is used to update the probability distributions of model parameters based on prior beliefs and available data.","category":"page"},{"location":"manual/bayesianupdating/#Bayes'-Theorem","page":"Bayesian Updating","title":"Bayes' Theorem","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Bayes' theorem is defined as","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"P(thetaY) = fracP(Ytheta)P(theta)P(Y)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"where P(theta) is the prior distribution, describing prior belief on theta. P(Ytheta) is the likelihood function evaluating how the data Y supports our belief. This is a function of theta not Y. P(thetaY) is called the posterior distribution and expresses an updated belief under data Y. The term P(Y), often called the marginal likelihood, or the evidence. It can be calculated as the integral of the likelihood multiplied by the prior distribution over the sample space of theta","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"P(Y) = intP(Ytheta)P(theta) dtheta","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"This term serves as a normalizing constant for the posterior probability. However, as it can be difficult or even impractical to calculate it is often disregarded. Instead, only the product of likelihood and prior is used, as it is proportional to the posterior probability","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"P(thetaY) propto P(Ytheta)P(theta)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Based on this relationship, the posterior probability can be approximated without calculation of P(Y) using a variety of sampling methods. Classic approaches such as rejection sampling can be inefficient, especially for multivariate cases due to high rejection rates. Instead, Metropolis et al., proposed the use of Markov chains to increase efficiency [17].","category":"page"},{"location":"manual/bayesianupdating/#Markov-Chain-Monte-Carlo","page":"Bayesian Updating","title":"Markov Chain Monte Carlo","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Markov chains are sequences of variables, where each variable is dependent on the last. In a discrete space Omega the series of random variables X_1X_2ldotsX_t is called a Marko chain if","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"p(X_t=x_tX_t-1=x_t-1ldotsX_1=x_1) = p(X_t=x_tX_t-1=x_t-1) ","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"A Markov chain is called ergodic or irreducible when it is possible to reach each state from every other state with a positive probability. Markov chains that are ergodic and time-homogeneous, i.e. the probability between states doesn't depend on time, and have a unique stationary distribution such that","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"pi(y) = sum_xinOmegaP(yx)pi(x)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The goal of Markov chain Monte Carlo (MCMC) sampling methods is to construct a Markov chain, whose stationary distribution is equal to the posterior distribution of Bayes' theorem. This will result in samples generated from the Markov chain being equivalent to random samples of the desired distribution. The very first MCMC algorithm is the Metropolis-Hastings (MH) Algorithm.","category":"page"},{"location":"manual/bayesianupdating/#Metropolis-Hastings","page":"Bayesian Updating","title":"Metropolis Hastings","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The Metropolis-Hastings algorithm, was published in 1970 by W. K. Hastings [18]. The MH algorithm is a random-walk algorithm that provides a selection criteria for choosing the next sample (theta_i + 1) in a Markov chain. This is done through a so-called proposal distribution q(theta_i + 1theta_i) which is well known and relatively easy to sample from. Usually, symmetric proposal distributions centred at (theta_i) are used, for example Normal and Uniform distributions. A candidate sample theta^* is sampled from the proposal distribution and accepted with probability alpha","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"alpha = minleft1fracP(theta^*Y)P(theta_iY)cdotfracq(theta_itheta^*)q(theta^*theta_i)right","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Substituting the posterior with Bayes' theorem yields","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"alpha = minleft1fracP(Ytheta^*)cdotP(theta^*)P(Y)P(Ytheta_i)cdotP(theta_i)P(Y)cdotfracq(theta_itheta^*)q(theta^*theta_i)right","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Note, how the normalization constant P(Y) cancels out. When the proposal is symmetric q(theta_itheta^*) = q(theta^*theta_i) the acceptance probability further simplifies to","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"alpha = minleft1fracP(theta^*Y)P(theta_iY)right","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"In practice, a random number r sim U(01) is sampled, and the candidate sample is accepted if a leq r","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"theta_i + 1 = theta^*     qquad  textif quad a leq r\ntheta_i + 1 = theta_i   qquad  textotherwise ","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"As an example consider a synthetic data sequence Y as the outcome of 100 Bernoulli trials with unknown success probability p (here p=0.8).","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"using UncertaintyQuantification # hide\n n = 100\n Y = rand(n) .<= 0.8\n return nothing # hide","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The likelihood function which, similar to a Model must accept a DataFrame, follows a Binomial distribution and returns the likelihood for each row in the DataFrame as a vector. The prior is chosen as a beta distribution with alpha=beta=1 (uniform on 0 1). It is often beneficial to use the log-likelihood and log-prior for numerical reasons.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"    function loglikelihood(df)\n            return [\n                sum(logpdf.(Binomial.(n, df_i.p), sum(Y))) for df_i in eachrow(df)\n            ]\n        end\n\nlogprior = df -> logpdf.(Beta(1,1), df.p)\nreturn nothing # hide","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"UncertaintyQuantification.jl implements a variant of the MH algorithm known as single-component Metropolis-Hastings, where the proposal and acceptance step is performed independently for each dimension. To run the algorithm, we must first define the SingleComponentMetropolisHastings object which requires the UnivariateDistribution as a proposal, a NamedTuple for x0 which defines the starting point of the Markov chain, the number of samples and the number of burn-in samples. The burn-in samples are used to start the chain but later discarded.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"    proposal = Normal(0, 0.2)\n    x0 = (;p=0.5)\n    n_samples= 4000\n    burnin = 500\n\n    mh = SingleComponentMetropolisHastings(proposal, x0, n_samples, burnin)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The final optional argument islog=true can be omitted when passing the log-likelihood and log-prior. When set to false, the algorithm will log for both the likelihood and prior. Finally, the algorithm is executed using the bayesianupdating function. This function returns the samples and the average acceptance rate.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"\nmh_samples, α   = bayesianupdating(logprior, loglikelihood, mh)\nreturn nothing # hide","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The following figure shows a histogram of the samples returned by the Metropolis-Hastings algorithm. For comparison, we also plot the analytical posterior distribution obtained using conjugate priors [19].","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"\nusing Plots # hide\np = histogram(mh_samples.p, normalize=:pdf, bin = 100, label = \"MH\"; xlabel=\"p\") # hide\n\nposterior = Beta(1+ sum(Y), 1 + n - sum(Y)) # hide\n\nx = range(0,1; length=200) # hide\ny = pdf.(posterior, x) # hide\n\nplot!(x, y, label=\"analytical posterior\", linewidth=2) # hide\n\nxlims!(0.5, 1.0) # hide\nreturn p # hide","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"As a second example we will attempt to sample from a bimodal target distribution in two dimensions. The prior is uniform over -2 2 in each dimension and the likelihood is a mixture of two Gaussian functions centred at 05 05 and -05 -05.  The standard deviation for both Gaussians are identical and if small enough will effectively disconnect the two functions.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"using UncertaintyQuantification # hide\nusing Plots # hide\nusing DataFrames # hide\nprior = Uniform(-2, 2)\nlogprior = df -> logpdf.(prior, df.x) .+ logpdf.(prior, df.y)\n\nN1 = MvNormal([-0.5, -0.5], 0.1)\n\nN2 = MvNormal([0.5, 0.5], 0.1)\n\nloglikelihood =\n    df -> log.([0.5 * pdf(N1, collect(x)) + 0.5 * pdf(N2, collect(x)) for x in eachrow(df)])\n\nn = 2000\nburnin = 500\n\nx0 = (; x=0.0, y=0.0)\n\nproposal = Normal()\n\nmh = SingleComponentMetropolisHastings(proposal, x0, n, burnin)\n\nmh_samples, α = bayesianupdating(logprior, loglikelihood, mh)\n\nscatter(mh_samples.x, mh_samples.y; lim=[-2, 2], legend = :none)\n\nxs = range(-2, 2, length = 100); ys = range(-2, 2, length = 100) # hide\nsample_points = reduce(vcat,[[x y] for x in xs, y in ys][:]) # hide\ndf_points = DataFrame(sample_points, :auto) # hide\nlikelihood_eval = exp.(loglikelihood(df_points)) # hide\ncontour!(xs, ys, likelihood_eval, lim = [-2,2], legend = :none) # hide\n","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The scatter plot clearly shows that the MH algorithm has converged to only one of the two peaks of the bimodal target (contour also plotted). In fact, this is a known weakness of the MH algorithm. However, there are a number of alternative MCMC methods that aim to solve this problem. One of these methods, known as Transitional Markov Chain Monte Carlo [20], will be presented next.","category":"page"},{"location":"manual/bayesianupdating/#Transitional-Markov-Chain-Monte-Carlo","page":"Bayesian Updating","title":"Transitional Markov Chain Monte Carlo","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The Transitional Markov Chain Monte Carlo (TMCMC) method [20] is an extension of the MH algorithm where instead of directly sampling a complex posterior distribution, samples are obtained from a series of simpler transitional distributions. The samples are obtained from independent single-step Markov Chains. The transitional distributions are defined as","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"    P^j propto P(Ytheta)^beta_j cdot P(theta)","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"where j in 1 ldots m  is the number of the transition step and beta_j is a tempering parameter with beta_1  cdots beta_m =1. This enables a slow transition from the prior to the posterior distribution. An important part of the TMCMC algorithm is that the tempering parameter has to be selected as to ensure the transition is smooth and gradual. The algorithm's authors suggest choosing the parameter such that a coefficient of variation of 100% is maintained in the likelihood P(Ytheta_i)^beta_j-beta_j - 1. At each level j the starting points for the independent Markov Chains are randomly samples (with replacement) from the current set of samples using statistical weights","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"w(theta_i) = fracP(Ytheta_i)^beta_j-beta_j-1sum_i=1^N P(Ytheta_i)^beta_j-beta_j-1","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The complete TMCMC algorithm can be summarized as","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Set j=0 and beta_j=0. Sample theta_i sim P(theta).\nSet j = j+1.\nCompute the next tempering parameter beta_j.\nDetermine the weights w(theta_i).\nGenerate a single-step Markov chain for each theta_i.\nRepeat steps (2) to (5) until (and including) (beta_j=1).","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"Returning to the bimodal example, this time using the TMCMC algorithm. In order to apply a different MCMC algorithm we only need to construct a TransitionalMarkovChainMonteCarlo object and pass it to the bayesianupdating method. The definition of prior and likelihood remains the same. In difference to the SingleComponentMetropolisHastings the log evidence is returned instead of the acceptance rate.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"\ntmcmc = TransitionalMarkovChainMonteCarlo(RandomVariable.(Uniform(-2,2), [:x, :y]), n, burnin)\n\ntmcmc_samples, S = bayesianupdating(logprior, loglikelihood, tmcmc)\n\nscatter(tmcmc_samples.x, tmcmc_samples.y; lim=[-2, 2], label=\"TMCMC\")","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"The resulting scatter plot shows how TMCMC is able to sample both peaks of the bimodal target distribution. The standard implementation of TMCMC uses a multivariate Gaussian proposal distribution centred at each theta_i with covariance matrix Sigma estimated from the current likelihood scaled by a factor beta^2. This scaling factor defaults to 02 as suggested by the authors, but can optionally be passed to the constructor as a fourth argument. Application of different MCMC Algorithms nested in the TMCMC give rise to variants of the algorithm. For example, it is possible to use the previously introduced SingleComponentMetropolisHastings resulting in SingleComponentTransitionalMarkovChainMonteCarlo.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"note: Note\nSingleComponentTransitionalMarkovChainMonteCarlo is currently not available but planned for implementation.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"For convenience, the prior can be automatically constructed from the random variables passed to TransitionalMarkovChainMonteCarlo.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"tmcmc = TransitionalMarkovChainMonteCarlo(RandomVariable.(Uniform(-2,2), [:x, :y]), n, burnin)\n\ntmcmc_samples, S = bayesianupdating(loglikelihood, tmcmc)","category":"page"},{"location":"manual/bayesianupdating/#Bayesian-calibration-of-computer-simulations","page":"Bayesian Updating","title":"Bayesian calibration of computer simulations","text":"","category":"section"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"UncertaintyQuantification.jl allows for complex computer models to be included in the Bayesian updating procedure, if for example one wishes to infer unknown model parameters from experimental data of model outputs. Several models can be evaluated in order to compute the likelihood function, by passing a vector of UQModels to the bayesianupdating method. These will be executed before the likelihood is evaluated and models outputs will be available in the DataFrame inside the likelihood function. There is no restriction on the type of UQModel used. For example, it is possible to use an ExternalModel and call an external solver.","category":"page"},{"location":"manual/bayesianupdating/","page":"Bayesian Updating","title":"Bayesian Updating","text":"For a complete example refer to the Inverse eigenvalue problem.","category":"page"},{"location":"api/simulations/#Simulations","page":"Simulations","title":"Simulations","text":"","category":"section"},{"location":"api/simulations/","page":"Simulations","title":"Simulations","text":"Various Monte Carlo based simulations for a wide range of applications.","category":"page"},{"location":"api/simulations/#Index","page":"Simulations","title":"Index","text":"","category":"section"},{"location":"api/simulations/","page":"Simulations","title":"Simulations","text":"Pages = [\"simulations.md\"]","category":"page"},{"location":"api/simulations/#Types","page":"Simulations","title":"Types","text":"","category":"section"},{"location":"api/simulations/","page":"Simulations","title":"Simulations","text":"SubSetSimulation\nSubSetInfinity","category":"page"},{"location":"api/simulations/#UncertaintyQuantification.SubSetSimulation","page":"Simulations","title":"UncertaintyQuantification.SubSetSimulation","text":"SubSetSimulation(n::Integer, target::Float64, levels::Integer, proposal::UnivariateDistribution)\n\nDefines the properties of a Subset simulation where n is the number of initial samples, target is the target probability of failure at each level, levels is the maximum number of levels and proposal is the proposal distribution for the markov chain monte carlo.\n\nExamples\n\njulia> SubSetSimulation(100, 0.1, 10, Uniform(-0.2, 0.2))\nSubSetSimulation(100, 0.1, 10, Uniform{Float64}(a=-0.2, b=0.2))\n\nReferences\n\n[14]\n\n\n\n\n\n","category":"type"},{"location":"api/simulations/#UncertaintyQuantification.SubSetInfinity","page":"Simulations","title":"UncertaintyQuantification.SubSetInfinity","text":"SubSetInfinity(n::Integer, target::Float64, levels::Integer, s::Real)\n\nDefines the properties of a Subset-∞ simulation where n is the number of initial samples, target is the target probability of failure at each level, levels is the maximum number of levels and s is the standard deviation for the proposal samples.\n\nExamples\n\njulia> SubSetInfinity(100, 0.1, 10, 0.5)\nSubSetInfinity(100, 0.1, 10, 0.5)\n\nReferences\n\n[16]\n\n[21]\n\n\n\n\n\n","category":"type"},{"location":"api/polyharmonicspline/#PolyharmonicSpline","page":"PolyharmonicSpline","title":"PolyharmonicSpline","text":"","category":"section"},{"location":"api/polyharmonicspline/#Index","page":"PolyharmonicSpline","title":"Index","text":"","category":"section"},{"location":"api/polyharmonicspline/","page":"PolyharmonicSpline","title":"PolyharmonicSpline","text":"    Pages = [\"polyharmonicspline.md\"]","category":"page"},{"location":"api/polyharmonicspline/#Type","page":"PolyharmonicSpline","title":"Type","text":"","category":"section"},{"location":"api/polyharmonicspline/","page":"PolyharmonicSpline","title":"PolyharmonicSpline","text":"PolyharmonicSpline","category":"page"},{"location":"api/polyharmonicspline/#UncertaintyQuantification.PolyharmonicSpline","page":"PolyharmonicSpline","title":"UncertaintyQuantification.PolyharmonicSpline","text":"PolyharmonicSpline(data::DataFrame, k::Int64, output::Symbol)\n\nCreates a polyharmonic spline that is trained by given data.\n\n#Examples\n\njulia> data = DataFrame(x = 1:10, y = [1, -5, -10, -12, -8, -1, 5, 12, 23, 50]);\n\njulia> PolyharmonicSpline(data, 2, :y) |> DisplayAs.withcontext(:compact => true)\nPolyharmonicSpline([1.14733, -0.449609, 0.0140379, -1.02859, -0.219204, 0.900367, 0.00895592, 1.07145, -5.33101, 3.88628], [-112.005, 6.84443], [1.0; 2.0; … ; 9.0; 10.0;;], 2, [:x], :y)\n\n\n\n\n\n","category":"type"},{"location":"api/polyharmonicspline/#Functions","page":"PolyharmonicSpline","title":"Functions","text":"","category":"section"},{"location":"api/polyharmonicspline/","page":"PolyharmonicSpline","title":"PolyharmonicSpline","text":"evaluate!(ps::PolyharmonicSpline, df::DataFrame)","category":"page"},{"location":"api/polyharmonicspline/#UncertaintyQuantification.evaluate!-Tuple{PolyharmonicSpline, DataFrame}","page":"PolyharmonicSpline","title":"UncertaintyQuantification.evaluate!","text":"evaluate!(ps::PolyharmonicSpline, df::DataFrame)\n\nEvaluate given data using a previously contructed PolyharmonicSpline metamodel.\n\n#Examples\n\njulia> data = DataFrame(x = 1:10, y = [1, -5, -10, -12, -8, -1, 5, 12, 23, 50]);\n\njulia> ps = PolyharmonicSpline(data, 2, :y);\n\njulia> df = DataFrame( x = [2.5, 7.5, 12, 30]);\n\njulia> evaluate!(ps, df);\n\njulia> df.y |> DisplayAs.withcontext(:compact => true)\n4-element Vector{Float64}:\n  -7.75427\n   8.29083\n  84.4685\n 260.437\n\n\n\n\n\n","category":"method"},{"location":"api/slurm/#SlurmInterface","page":"SlurmInterface","title":"SlurmInterface","text":"","category":"section"},{"location":"api/slurm/#Index","page":"SlurmInterface","title":"Index","text":"","category":"section"},{"location":"api/slurm/","page":"SlurmInterface","title":"SlurmInterface","text":"Pages = [\"slurm.md\"]","category":"page"},{"location":"api/slurm/#Type","page":"SlurmInterface","title":"Type","text":"","category":"section"},{"location":"api/slurm/","page":"SlurmInterface","title":"SlurmInterface","text":"SlurmInterface","category":"page"},{"location":"api/slurm/#UncertaintyQuantification.SlurmInterface","page":"SlurmInterface","title":"UncertaintyQuantification.SlurmInterface","text":"SlurmInterface(options::Dict{String,String}, throttle::Integer, btachsize::Integer, extras::Vector{String})\n\nWhen SlurmInterface is passed to an ExternalModel, model evaluations are executed using slurm job arrays. This allows for heavier simulations or workflows to be sampled, without relying on Julia's native parallelism. SlurmInterface automatically generates a slurm job array script, and Julia waits for this job to finish before extracting results.\n\nWhen using SlurmInterface, you no longer need to load workers into Julia with addprocs(N), and the requested nodes / tasks those required by individual model evaluations. Use extras to specify anything that must be preloaded for your models to be executed (for example loading modules).\n\nThe throttle specifies the number of simulations in the job array which are run concurrently. I.e., if you perform MonteCarlo simulation with N=1000 samples, with throttle=200, it will run 1000 simulations in total, but only 200 at the same time. Your HPC scheduler (and admin) may be unhappy if you request too many concurrent jobs. If left empty, you scheduler's default throttle will be used. In the case that your HPC machine limits the size of submitted job arrays, you can split the submissions into smaller \"batches\". Specify \"batchsize\" to the maximum size of a job array. This does not change the total number of runs.\n\nparameters\n\noptions   : A dictionary of SBATCH options to add to the slurm script\nthrottle  : the number of jobs to be run at the same time\nbatchsize : maximum size of the slurm array, use when HPC limits the number of jobs in arrays\nextras    : instructions to be executed before the model is run, e.g. activating a python environment or loading modules\n\nExamples\n\njulia> slurm = SlurmInterface(Dict(\"account\" => \"HPC_account_1\", \"partition\" => \"CPU_partition\"), extras = [\"load python3\"])\nSlurmInterface(Dict(\"account\" => \"HPC_account_1\", \"partition\" => \"CPU_partition\"), 0, 0, [\"load python3\"])\n\n\n\n\n\n","category":"type"},{"location":"manual/gettingstarted/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Here we introduce the basic building blocks of UncertaintyQuantification. This includes the inputs such as Parameter or RandomVariable which will feed into any Model for a variety of different analyses. We will also present more advanced concepts including how to model dependencies between the inputs through copulas.","category":"page"},{"location":"manual/gettingstarted/#Inputs","page":"Getting Started","title":"Inputs","text":"","category":"section"},{"location":"manual/gettingstarted/#Parameters","page":"Getting Started","title":"Parameters","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A Parameter is defined as a constant scalar value. In addition to value the constructor also requires a Symbol by which it can later be identified in the Model. A Symbol is a Julia object which is often used as a name or label. Symbols are defined using the : prefix. Parameters represent constant deterministic values. As an example we define a Parameter representing the gravity of Earth.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"using UncertaintyQuantification # hide\ng = Parameter(9.81, :g)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Parameters are very handy when constants show up in the Model in multiple spaces. Instead of updating every instance in the Model, we can conveniently update the value by changing a single line.","category":"page"},{"location":"manual/gettingstarted/#Random-Variables","page":"Getting Started","title":"Random Variables","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A RandomVariable is essentially a wrapper around any UnivariateDistribution defined in the Distributions.jl package [6]. Similarly to the Parameter, the second argument to the constructor is a Symbol acting as a unique identifier. For example, a standard gaussian random variable is defined by passing Normal() and :x as arguments.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"x = RandomVariable(Normal(), :x)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A list of all possible distributions can be generated by executing subtypes(UnivariateDistribution) in the Julia REPL (read-eval-print loop). Note that, Distributions is re-exported from UncertaintyQuantification and no separate using statement is necessary. In addition, the most important methods of the UnivariateDistribution including pdf, cdf, and quantile, are also defined for the RandomVariable.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Random samples can be drawn from a RandomVariable by calling the sample method passing the random variable and the desired number of samples.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"samples = sample(x, 100) # sample(x, MonteCarlo(100))\nreturn nothing # hide","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"The sample method returns a DataFrame with the samples in a single column. When sampling from a Vector of random variables these individual columns are automatically merged into one unified DataFrame. By default, this will use standard Monte Carlo simulation to obtain the samples. Alternatively, any of the quasi-Monte Carlo methods can be used instead.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"samples = sample(x, SobolSampling(100))\nsamples = sample(x, LatinHypercubeSampling(100))\nreturn nothing # hide","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Many of the advanced simulations, e.g. line sampling or subset simulation require mappings to (and from) the standard normal space, and these are exposed through the to_standard_normal_space! and to_physical_space! methods respectively. These operate on a DataFrame and as such can be applied to samples directly. The transformation is done in-place, i.e. no new DataFrame is returned. As such, in the following example, the samples end up exactly as they were in the beginning.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"to_standard_normal_space!(x, samples)\nto_physical_space!(x, samples)","category":"page"},{"location":"manual/gettingstarted/#Dependencies","page":"Getting Started","title":"Dependencies","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"UncertaintyQuantification supports modelling of dependencies through copulas. By using copulas, the modelling of the dependence structure is separated from the modelling of the univariate marginal distributions. The basis for copulas is given by Sklar's theorem [7]. It states that any multivariate distribution H in dimensions d geq 2 can be separated into its marginal distributions F_i and a copula function C.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"H(x_1ldotsx_2) = C(F_1(x_1)ldotsF_d(x_d))","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"For a thorough discussion of copulas, see [8].","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"In line with Sklar's theorem we build the joint distribution of two dependent random variables by separately defining the marginal distributions.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"using UncertaintyQuantification # hide\nx = RandomVariable(Normal(), :x)\ny = RandomVariable(Uniform(), :y)\nmarginals = [x, y]\nreturn nothing # hide","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Next, we define the copula to model the dependence. UncertaintyQuantification supports Gaussian copulas for multivariate d geq 2 dependence. Here, we define a Gaussian copula by passing the correlation matrix and then build the JointDistribution from the copula and the marginals.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"cop = GaussianCopula([1 0.8; 0.8 1])\njoint = JointDistribution(marginals, cop)\nreturn nothing # hide","category":"page"},{"location":"manual/gettingstarted/#Models","page":"Getting Started","title":"Models","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"In this section we present the models included in UncertaintyQuantification. A model, in its most basic form, is a relationship between a set of input variables x in mathbbR^n_x and an output y in mathbbR. Currently, most models are assumed to return single-valued outputs. However, as seen later, the ExternalModel is capable of extracting an arbitrary number of outputs from a single run of an external solver.","category":"page"},{"location":"manual/gettingstarted/#Model","page":"Getting Started","title":"Model","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A Model is essentially a native Julia function operating on the previously defined inputs. Building a Model requires two things: a Function, which is internally passed a DataFrame containing the samples and must return a Vector containing the model response for each sample, and a Symbol which is the identifier used to add the model output into the DataFrame.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Suppose we wanted to define a Model which computes the distance from the origin of two variables x and y as z. We first define the function and then pass it to the Model.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"using UncertaintyQuantification, DataFrames # hide\nfunction z(df::DataFrame)\n  return @. sqrt(df.x^2 + df.y^2)\nend\nm = Model(z, :z)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"An alternative for a simple model such as this, is to directly pass an anonymous function to the Model.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"m = Model(df -> sqrt.(df.x.^2 .+ df.y.^2), :z)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"After defining it, a Model can be evaluated on a set of samples by calling the evaluate! method. This will add the model outcome to the DataFrame. Alternatively, the reponse can be obtained as a vector, by calling the Model as a function.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"samples = sample([x, y], MonteCarlo(1000))\nevaluate!(m, samples) # add to the DataFrame\noutput = m(samples) # return a Vector","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"However, most of the time manual evaluation of the Model will not be necessary as it is done internally by whichever analysis is performed.","category":"page"},{"location":"manual/gettingstarted/#ParallelModel","page":"Getting Started","title":"ParallelModel","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"With the basic Model it is up to the user to implement an efficient function which returns the model responses for all samples simultaneously. Commonly, this will involve vectorized operations as presented in the example. For more complex or longer running models, UncertaintyQuantification provides a simple ParallelModel. This model relies on the capabilites of the Distributed module, which is part of the standard library shipped with Julia. Without any present workers, the ParallelModel will evaluate its function in a loop for each sample. If one or more workers are present, it will automatically distribute the model evaluations. For this to work, UncertaintyQuantification must be loaded with the @everywhere macro in order to be loaded on all workers. In the following example, we first load Distributed and add four local workers. A simple model is then evaluated in parallel. Finally, the workers are removed.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"using Distributed\naddprocs(4) # add 4 local workers\n\n@everywhere using UncertaintyQuantification\n\nx = RandomVariable(Normal(), :x)\ny = RandomVariable(Normal(), :y)\n\nm = ParallelModel(df -> sqrt(df.x^2 .+ df.y^2), :z)\n\nsamples = sample([x, y], 1000)\nevaluate!(m, samples)\n\nrmprocs(workers()) # release the local workers","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"It is important to note, that the ParallelModel requires some overhead to distribute the function calls to the workers. Therefore it performs significantly slower than the standard Model with vectorized operations for a simple function as in this example.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"By using ClusterManagers.jl to add the workers, the ParallelModel can easily be run on an existing compute cluster such as Slurm.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"note: Note\nFor heavier external models or workflows in parallel on compute clusters, using SlurmInterface is recommended. See High Performance Computing.","category":"page"},{"location":"manual/gettingstarted/#ExternalModel","page":"Getting Started","title":"ExternalModel","text":"","category":"section"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"The ExternalModel provides interaction with almost any third-party solver. The only requirement is, that the solver uses text-based input and output files in which the values sampled from the random variables can be injected for each individual run. The output quantities are then extracted from the files generated by the solver using one (or more) Extractor(s). This way, the simulation techniques included in this module, can be applied to advanced models in finite element software such as OpenSees or Abaqus.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"The first step in building the ExternalModel is to define the folder where the source files can be found as well as the working directory. Here, we assume that the source file for a simple supported beam model is located in a subdirectory of our current working directory. Similarly, the working directory for the solver is defined. In addition, we define the exact files where values need to be injected, and any extra files required. No values will be injected into the files specified as extra. In this example, no extra files are needed, so the variable is defined as an empty String vector.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"sourcedir = joinpath(pwd(), \"demo/models\")\nsourcefiles = [\"supported-beam.tcl\"]\nextrafiles = String[]\nworkdir = joinpath(pwd(), \"supported-beam\")","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Next, we must define where to inject values from the random variables and parameters into the input files. For this, we make use of the Mustache.jl and Format.jl modules. The values in the source file must be replaced by triple curly bracket expressions of the form {{{ :x }}},  where :x is the identifier of the RandomVariable or Parameter to be injected. For example, to inject the Young's modulus and density of an elastic isotropic material in OpenSees, one could write the following.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"nDMaterial ElasticIsotropic 1 {{{ :E }}} 0.25 {{{ :rho }}}","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"This identifies where to inject the values, but not in which format. For this reason, we define a Dict{Symbol, String} which maps the identifiers of the inputs to a Python-style format string. In order to inject our values in scientific notation with eight digits, we use the format string \".8e\". For any not explicitly defined Symbol we can include :* as a fallback.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"formats = Dict(:E => \".8e\",:rho => \".8e\", :* => \".12e\")","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"After formatting and injecting the values into the source file, it would look similar to this.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"nDMaterial ElasticIsotropic 1 9.99813819e+02 0.25 3.03176259e+00","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"Now that the values are injected into the source files, the next step is to extract the desired output quantities. This is done using an Extractor. The Extractor is designed similarly to the Model in that it takes a Function and a Symbol as its parameters. However, where a DataFrame is passed to the Model, the working directoy for the currently evaluated sample is passed to the function of the Extractor. The user defined function must then extract the required values from the file and return them. Here, we make use of the DelimitedFiles module to extract the maximum absolute displacement from the output file that OpenSees generated.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"disp = Extractor(base -> begin\n  file = joinpath(base, \"displacement.out\")\n  data = readdlm(file, ' ')\n\n  return maximum(abs.(data[:, 2]))\nend, :disp)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"An arbitrary number of Extractor functions can be defined in order to extract multiple output values from the solver.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"The final step before building the model is to define the solver. The solver requires the path to the binary, and the input file. Optional command line arguments can be passed to the Solver through the args keyword. If the solver binary is not on the system path, the full path to the executable must be defined. Finally, the ExternalModel is assembled.","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"opensees = Solver(\n  \"OpenSees\",\n  \"supported-beam.tcl\";\n  args = \"\"\n)\n\next = ExternalModel(\n  sourcedir, sourcefiles, disp, opensees; formats=numberformats, workdir=workdir, extras=extrafiles\n)","category":"page"},{"location":"manual/gettingstarted/","page":"Getting Started","title":"Getting Started","text":"A full example of how to run a reliability analysis of a model defined in OpenSees can be found in the demo files.","category":"page"},{"location":"api/responsesurface/#ResponseSurface","page":"ResponseSurface","title":"ResponseSurface","text":"","category":"section"},{"location":"api/responsesurface/#Index","page":"ResponseSurface","title":"Index","text":"","category":"section"},{"location":"api/responsesurface/","page":"ResponseSurface","title":"ResponseSurface","text":"    Pages = [\"responcesurface.md\"]","category":"page"},{"location":"api/responsesurface/#Type","page":"ResponseSurface","title":"Type","text":"","category":"section"},{"location":"api/responsesurface/","page":"ResponseSurface","title":"ResponseSurface","text":"ResponseSurface","category":"page"},{"location":"api/responsesurface/#UncertaintyQuantification.ResponseSurface","page":"ResponseSurface","title":"UncertaintyQuantification.ResponseSurface","text":"ResponseSurface(data::DataFrame, dependendVarName::Symbol, deg::Int, dim::Int)\n\nCreates a response surface using polynomial least squares regression with given degree.\n\nExamples\n\njulia> data = DataFrame(x = 1:10, y = [1, 4, 10, 15, 24, 37, 50, 62, 80, 101]);\n\njulia> rs = ResponseSurface(data, :y, 2) |> DisplayAs.withcontext(:compact => true)\nResponseSurface([0.483333, -0.238636, 1.01894], :y, [:x], 2, Monomial{Commutative{CreationOrder}, Graded{LexOrder}}[1, x₁, x₁²])\n\n\n\n\n\n","category":"type"},{"location":"api/responsesurface/#Functions","page":"ResponseSurface","title":"Functions","text":"","category":"section"},{"location":"api/responsesurface/","page":"ResponseSurface","title":"ResponseSurface","text":"evaluate!(rs::ResponseSurface, data::DataFrame)","category":"page"},{"location":"api/responsesurface/#UncertaintyQuantification.evaluate!-Tuple{ResponseSurface, DataFrame}","page":"ResponseSurface","title":"UncertaintyQuantification.evaluate!","text":"evaluate!(rs::ResponseSurface, data::DataFrame)\n\nevaluating data by using a previously trained ResponseSurface.\n\nExamples\n\njulia> data = DataFrame(x = 1:10, y = [1, 4, 10, 15, 24, 37, 50, 62, 80, 101]);\n\njulia> rs = ResponseSurface(data, :y, 2);\n\njulia> df = DataFrame(x = [2.5, 11, 15]);\n\njulia> evaluate!(rs, df);\n\njulia> df.y |> DisplayAs.withcontext(:compact => true)\n3-element Vector{Float64}:\n   6.25511\n 121.15\n 226.165\n\n\n\n\n\n","category":"method"},{"location":"#UncertaintyQuantification.jl","page":"Home","title":"UncertaintyQuantification.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for uncertainty quantification. Current functionality includes:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Simulation-based reliability analysis\nMonte Carlo simulation\nQuasi Monte Carlo simulation (Sobol, Halton)\nLine Sampling\nSubset Simulation\nSensitivity analysis\nGradients\nSobol indices\nMetamodeling\nPolyharmonic splines\nResponse Surface\nThird-party solvers\nConnect to any solver by injecting random samples into source files\nHPC interfacing with slurm","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the latest release through the Julia package manager run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add UncertaintyQuantification\njulia> using UncertaintyQuantification","category":"page"},{"location":"","page":"Home","title":"Home","text":"or install the latest development version with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add UncertaintyQuantification#master\njulia> using UncertaintyQuantification","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Jasper Behrensdorf, Institute for Risk and Reliability, Leibniz University Hannover\nAnder Gray, Institute for Risk and Uncertainty, University of Liverpool","category":"page"},{"location":"#Related-packages","page":"Home","title":"Related packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"OpenCossan: Matlab-based toolbox for uncertainty quantification and management","category":"page"}]
}
